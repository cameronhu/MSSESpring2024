{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Started with Chatbot Arena Dataset**\n",
    "\n",
    "This notebook provide starter code loading and inspecting the core dataset and the auxiliary datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversation Data**\n",
    "\n",
    "The source dataset comes from https://huggingface.co/datasets/lmsys/chatbot_arena_conversations. The author describes the dataset as follows:\n",
    "\n",
    "> This dataset contains 33K cleaned conversations with pairwise human preferences. It is collected from 13K unique IP addresses on the Chatbot Arena from April to June 2023. Each sample includes a question ID, two model names, their full conversation text in OpenAI API JSON format, the user vote, the anonymized user ID, the detected language tag, the OpenAI moderation API tag, the additional toxic tag, and the timestamp.\n",
    "\n",
    "[Chatbot Arena](https://chat.lmsys.org/) is a platform where users can ask questions and two chatbots will provide answers. The user then votes on which chatbot provided the best answer. The dataset contains the questions, the answers, and the user votes.\n",
    "\n",
    "You can find the processed dataset in `./chatbot-arena-conversations.jsonl.gz`. The dataset is in JSON line format and compressed using gzip. It has gone through the following preprocessing steps to make analysis easier:\n",
    "\n",
    "- Removed non-English conversations.\n",
    "- Removed conversations with more than one round.\n",
    "- Removed conversations classified as toxic or harmful.\n",
    "\n",
    "The dataset you will be working with contains `25322` rows (out of `33000` total rows) and `7` columns. The columns are:\n",
    "\n",
    "- `question_id`: A unique identifier for the question.\n",
    "- `model_a`: The name of the first chatbot model.\n",
    "- `model_b`: The name of the second chatbot model.\n",
    "- `winner`: The name of the chatbot model that won the user vote.\n",
    "- `judge`: The anonymized user ID that voted.\n",
    "- `conversation_a`: The conversation between the user and `model_a`.\n",
    "- `conversation_b`: The conversation between the user and `model_b`.\n",
    "\n",
    "An example of the data is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question_id\": \"c0fc42c6f5f14f2aa5a89f71f8553730\",\n",
    "  \"model_a\": \"vicuna-13b\",\n",
    "  \"model_b\": \"koala-13b\",\n",
    "  \"winner\": \"model_a\",\n",
    "  \"judge\": \"arena_user_973\",\n",
    "  \"conversation_a\": [\n",
    "    {\n",
    "      \"content\": \"Count from 1 to 10 with step = 3\",\n",
    "      \"role\": \"user\"\n",
    "    },\n",
    "    {\n",
    "      \"content\": \"1, 4, 7, 10\\n\\nCounting with a step of 3 means that you will increase the number by 3 each time. So, the first number is 1, the next is 4 (1 + 3), the next is 7 (4 + 3), and so on.\",\n",
    "      \"role\": \"assistant\"\n",
    "    }\n",
    "  ],\n",
    "  \"conversation_b\": [\n",
    "    {\n",
    "      \"content\": \"Count from 1 to 10 with step = 3\",\n",
    "      \"role\": \"user\"\n",
    "    },\n",
    "    {\n",
    "      \"content\": \"1, 4, 7, 10\",\n",
    "      \"role\": \"assistant\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Let's investigate the conversation data first. It is in JSON line format, compressed with `gzip`. You can load the data with `pd.read_json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\n",
    "    \"/home/jovyan/shared/sp24_grad_project_data/nlp-arena-data/chatbot-arena-conversations.jsonl.gz\",\n",
    "    lines=True,\n",
    ")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the length distribution of the prompt. Do the arena users ask long or short questions? What are the average length of input for the chatbots?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prompt\"] = df[\"conversation_a\"].str[0].str[\"content\"]\n",
    "df[\"prompt\"].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prompt_length\"] = df[\"prompt\"].str.len()\n",
    "df[\"prompt_length\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the mean length of the prompt is about 200 characters, while the _median_ is 72 characters. This suggests that the distribution is right-skewed! Let's visualize this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the length of the prompt\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[\"prompt_length\"], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding Data**\n",
    "\n",
    "Recall from homework 3, text embedding models transform natural language text into numerical vectors. The vector are generated in such a way that semantically similar text are close to each other in the vector space. In real world, these embeddings to find similar questions or to cluster the questions.\n",
    "\n",
    "Concretely, `./chatbot-arena-prompts-embeddings.npy` contains the 256 dimensional text embeddings for each of the human question. The embeddings are generated using OpenAI's `text-embedding` model. We will explain what is embedding and how can you use them later in this notebook. It has shape (25322, 256) and have dtype('float32').\n",
    "\n",
    "Let's try to compute the similarity using dot product of the first 1000 embeddings, and come up with examples of similar questions given the first question. This is am example of what you can do with embedding data. The ordering of the embeddings is the same as the ordering of the questions in the conversation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(\n",
    "    \"/home/jovyan/shared/sp24_grad_project_data/nlp-arena-data/chatbot-arena-prompts-embeddings.npy\"\n",
    ")\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to find the closest prompt to a given prompt\n",
    "\n",
    "embeddings_sample = embeddings[:1000]\n",
    "\n",
    "# compute the dot product between the embeddings\n",
    "dot_product = np.dot(embeddings_sample, embeddings_sample.T)\n",
    "dot_product.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the most similar questions to the a sample question. We can leverage the dot product results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_prompt_idx = 23\n",
    "source_prompt = df.iloc[source_prompt_idx].prompt\n",
    "source_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "similar_promts_idx = np.argsort(dot_product[source_prompt_idx])[-top_k:][::-1]\n",
    "similar_promts = df.iloc[similar_promts_idx].prompt\n",
    "similar_promts.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Modeling and Hardness Score Data**\n",
    "\n",
    "We used OpenAI's GPT 3.5 model to evaluate the difficulty of the questions. The model provides a hardness score from 1 to 10 for each question. The model also provides a topic modeling for each question. The topic modeling is a two-word description of the topic of the question. The topic modeling and hardness score can be used to understand the difficulty of the questions and to group similar questions together. Concretely:\n",
    "\n",
    "- `./chatbot-arena-gpt3-scores.jsonl.gz` contains labels for the dataset you can use for later modeling tasks. It has the following fields:\n",
    "  - `question_id`: The unique identifier for the question, as seen in `./chatbot-arena-conversations.jsonl.gz`.\n",
    "  - `prompt`: The extracted human question. This is equivalent to the first message in `conversation_a` and `conversation_b` in `./chatbot-arena-conversations.jsonl.gz`.\n",
    "  - `openai_scores_raw_choices_nested`: The response from OpenAI GPT 3.5 model (see later for the prompt). It contains the evaluated topic model, reason for a hardness score from 1 to 10, and the value. For each prompt, we have 3 responses from GPT 3.5 because it is a probablistic model. In fact, you will find in real world there are common multiple labels from different annotators for ground truth data. We extracted the fields into the following columns.\n",
    "  - `topic_modeling_1`, `topic_modeling_2`, `topic_modeling_3`: The topic modeling for the first, second, and third response. Each topic should have two words.\n",
    "  - `score_reason_1`, `score_reason_2`, `score_reason_3`: The reason for the hardness score for the first, second, and third response.\n",
    "  - `score_value_1`, `score_value_2`, `score_value_3`: The hardness score for the first, second, and third response.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question_id\": \"58210e39b3fd4441a2bd4a518bb44c2d\",\n",
    "  \"prompt\": \"What is the difference between OpenCL and CUDA?\",\n",
    "  \"openai_scores_raw_choices_nested\": [\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 0,\n",
    "      \"logprobs\": null,\n",
    "      \"message\": {\n",
    "        \"content\": \"{\\n    \\\"topic_modeling\\\": \\\"Technical Comparison\\\",\\n    \\\"score_reason\\\": \\\"This prompt requires the AI to accurately compare and contrast two distinct technologies, OpenCL and CUDA. It assesses the AI's factual accuracy and knowledge of these technologies, as well as its ability to articulate the differences between them.\\\",\\n    \\\"score_value\\\": 9\\n}\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"function_call\": null,\n",
    "        \"tool_calls\": null\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 1,\n",
    "      \"logprobs\": null,\n",
    "      \"message\": {\n",
    "        \"content\": \"{\\n    \\\"topic_modeling\\\": \\\"Software Comparison\\\",\\n    \\\"score_reason\\\": \\\"This prompt assesses the AI's factual accuracy in distinguishing between two similar but distinct software frameworks.\\\",\\n    \\\"score_value\\\": 8\\n}\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"function_call\": null,\n",
    "        \"tool_calls\": null\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 2,\n",
    "      \"logprobs\": null,\n",
    "      \"message\": {\n",
    "        \"content\": \"{\\n    \\\"topic_modeling\\\": \\\"Comparison, Technology\\\",\\n    \\\"score_reason\\\": \\\"This prompt requires the AI to demonstrate knowledge of two different technologies, compare their features, and explain their distinctions. This task assesses the AI's factual accuracy and proficiency in understanding complex technological concepts.\\\",\\n    \\\"score_value\\\": 9\\n}\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"function_call\": null,\n",
    "        \"tool_calls\": null\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"topic_modeling_1\": \"Technical Comparison\",\n",
    "  \"score_reason_1\": \"This prompt requires the AI to accurately compare and contrast two distinct technologies, OpenCL and CUDA. It assesses the AI's factual accuracy and knowledge of these technologies, as well as its ability to articulate the differences between them.\",\n",
    "  \"score_value_1\": 9,\n",
    "  \"topic_modeling_2\": \"Software Comparison\",\n",
    "  \"score_reason_2\": \"This prompt assesses the AI's factual accuracy in distinguishing between two similar but distinct software frameworks.\",\n",
    "  \"score_value_2\": 8,\n",
    "  \"topic_modeling_3\": \"Comparison, Technology\",\n",
    "  \"score_reason_3\": \"This prompt requires the AI to demonstrate knowledge of two different technologies, compare their features, and explain their distinctions. This task assesses the AI's factual accuracy and proficiency in understanding complex technological concepts.\",\n",
    "  \"score_value_3\": 9\n",
    "}\n",
    "```\n",
    "\n",
    "_Warning_: This data can be messy! This is intentionally not cleaned up for you to resemble real world data. You are responsible to figuring out the irregularities and cleaning it up. The following cells demostrate the example of messy data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness = pd.read_json(\n",
    "    \"/home/jovyan/shared/sp24_grad_project_data/nlp-arena-data/chatbot-arena-gpt3-scores.jsonl.gz\",\n",
    "    lines=True,\n",
    ")\n",
    "topic_and_hardness.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of messy data, there are other problems with the data but we are just demostrating one\n",
    "\n",
    "topic_and_hardness[\"score_value_1\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of messy data, there are other problems with the data but we are just demostrating one\n",
    "\n",
    "topic_and_hardness[\"score_value_1\"][topic_and_hardness[\"score_value_1\"].apply(\n",
    "    lambda x: isinstance(x, list))].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
