{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b6525a",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadcd13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, opt_method, learning_rate, batch_size, epoch, l2):\n",
    "        self.model = model\n",
    "        if opt_method == \"sgdm\":\n",
    "            self.optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum=0.9)\n",
    "        elif opt_method == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=l2)\n",
    "        else:\n",
    "            raise NotImplementedError(\"This optimization is not supported\")\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "    def train(self, train_data, draw_curve=True):\n",
    "        self.encoder = train_data.encoder\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        train_loss_list, train_acc_list = [], []\n",
    "        \n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        for n in tqdm(range(self.epoch), leave=False):\n",
    "            self.model.train()\n",
    "            epoch_loss, epoch_acc = 0.0, 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                batch_importance = y_batch.shape[0] / len(train_data)\n",
    "                hidden = self.model.init_hidden(y_batch.shape[0])\n",
    "                \n",
    "                # batch outputs\n",
    "                y_pred, _ = self.model(X_batch, hidden)\n",
    "                \n",
    "                # loss func\n",
    "                batch_loss = loss_func(y_pred, y_batch)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # record accuracy\n",
    "                batch_acc = torch.sum(torch.argmax(y_batch, axis=-1) == torch.argmax(y_pred, axis=-1))\n",
    "                \n",
    "                epoch_acc += batch_acc.detach().cpu().item() * batch_importance\n",
    "                epoch_loss += batch_loss.detach().cpu().item() * batch_importance\n",
    "            \n",
    "            train_acc_list.append(epoch_acc)\n",
    "            train_loss_list.append(epoch_loss)\n",
    "            \n",
    "        if draw_curve:\n",
    "            x_axis = np.arange(self.epoch)\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "            axes[0].plot(x_axis, train_loss_list, label=\"Train\")\n",
    "            axes[0].set_title(\"Loss\")\n",
    "            axes[0].legend()\n",
    "            axes[1].plot(x_axis, train_acc_list, label='Train')\n",
    "            axes[1].set_title(\"Accuracy\")\n",
    "            axes[1].legend()\n",
    "    \n",
    "    def sample(self, num_seq=10):\n",
    "        self.model.eval()\n",
    "        seqs = []\n",
    "        with torch.no_grad():\n",
    "            for _ in tqdm(range(num_seq), leave=False):\n",
    "                chars = ['SOS']\n",
    "                hidden = self.model.init_hidden(1)\n",
    "                while chars[-1] != 'EOS':\n",
    "                    input_encoding = self.encoder.transform(np.array([chars[-1]]).reshape(-1, 1)).toarray()\n",
    "                    input_encoding = torch.tensor(input_encoding, dtype=torch.float).reshape(1, 1, -1)\n",
    "                    out, hidden = self.model(input_encoding, hidden)\n",
    "\n",
    "                    prob = out.detach().numpy().flatten()\n",
    "                    prob /= np.sum(prob)\n",
    "\n",
    "                    index = np.random.choice(self.model.input_size, p=prob)\n",
    "                    out_encoding = np.zeros((1, self.model.input_size))\n",
    "                    out_encoding[0, index] = 1.0\n",
    "                    char = data.encoder.inverse_transform(out_encoding).flatten().tolist()[0]\n",
    "                    chars.append(char)\n",
    "                seqs.append(''.join(chars[1:-1]))\n",
    "        return seqs\n",
    "\n",
    "\n",
    "def validate(seq):\n",
    "    num = len(seq)\n",
    "    unique = set(seq)\n",
    "    valid = []\n",
    "    for s in unique:\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if mol is not None:\n",
    "            valid.append(s)\n",
    "            \n",
    "    print(f\"Number of unique SMILES: {len(unique)}\")\n",
    "    print(f\"Number of valid & unique SMILES: {len(valid)}\")\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bcf514",
   "metadata": {},
   "source": [
    "## (a)\n",
    "\n",
    "*For debugging*: Length of the vocabulary (i.e. unique characters) should be 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca08c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_smiles(path):\n",
    "    with open(path) as f:\n",
    "        smiles = f.read().split('\\n')\n",
    "    return smiles\n",
    "\n",
    "\n",
    "def pad_start_end_token(smiles):\n",
    "    \"\"\"\n",
    "    Pad a list of SMILES with \"SOS\" and \"EOS\" token\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles: list of str\n",
    "        A list containing SMILES strings to pad\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    padded: list of list of str\n",
    "        A list containing padded SMILES strings. Example: [['SOS', 'C', 'EOS'], ...]\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "smiles = load_smiles(...)\n",
    "padded_smiles = pad_start_end_token(smiles)\n",
    "\n",
    "vocab = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c84fdb",
   "metadata": {},
   "source": [
    "*For debugging*: Please execute the following block to test the vocabulary and padded smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f797a14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_pad():\n",
    "    assert padded_smiles[0] == ['SOS', 'C', 'EOS']\n",
    "    assert padded_smiles[1] == ['SOS', 'N', 'EOS']\n",
    "    assert len(vocab) == 17\n",
    "    print(\"Well done!\")\n",
    "\n",
    "test_pad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c3937",
   "metadata": {},
   "source": [
    "Finish the missing lines to do the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37383d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, smiles, vocab):\n",
    "        \n",
    "        self.vocab = np.array(vocab, dtype=str).reshape(-1, 1)\n",
    "        \n",
    "        self.encoder = ...\n",
    "        # fit the encoder\n",
    "        ...\n",
    "        \n",
    "        # one-hot encoding\n",
    "        self.data = ...\n",
    "        \n",
    "        self.data = nn.utils.rnn.pad_sequence(self.data, batch_first=True)\n",
    "        self.X = self.data[:, :-1, :]\n",
    "        self.y = self.data[:, 1:, :]\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(self.data.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    \n",
    "data = ... # initialize dataset\n",
    "input_size = data.vocab.shape[0] # should be 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d9a89",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88058ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = ...\n",
    "        self.hidden_size = ...\n",
    "        self.num_layers = ...\n",
    "        \n",
    "        self.rnn = ...\n",
    "        self.fc = ...\n",
    "        self.softmax = ...\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        ...\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191a478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ...\n",
    "trainer = ...\n",
    "trainer.train(data)\n",
    "\n",
    "# generate 1000 strings & validation with the `validate` function\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166bb460",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa5c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = ...\n",
    "        self.hidden_size = ...\n",
    "        self.num_layers = ...\n",
    "        \n",
    "        self.lstm = ...\n",
    "        self.fc = ...\n",
    "        self.softmax = ...\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        ...\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size), \n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f9d41e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ...\n",
    "trainer = ...\n",
    "trainer.train(data)\n",
    "\n",
    "# generate 1000 strings & validation with the `validate` function\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c142",
   "language": "python",
   "name": "c142"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
