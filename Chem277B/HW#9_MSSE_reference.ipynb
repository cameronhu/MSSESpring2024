{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff7c44",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e618aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MnistDataset():\n",
    "    def __init__(self, X, y, transform_X=lambda x: x):\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "        self.X = transform_X(X)\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    \n",
    "def load_mnist(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        train_data, test_data = pickle.load(f)\n",
    "    \n",
    "    X_train = torch.tensor(train_data[0], dtype=torch.float).unsqueeze(1)\n",
    "    y_train = torch.tensor(train_data[1], dtype=torch.long)\n",
    "    X_test = torch.tensor(test_data[0], dtype=torch.float).unsqueeze(1)\n",
    "    y_test = torch.tensor(test_data[1], dtype=torch.long)\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a414edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and normalize by dividing the maximum value\n",
    "X_train, y_train, X_test, y_test = ...\n",
    "X_train = ...\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further split the train to train/validation with 80/20 \n",
    "train_index, val_index = ...\n",
    "\n",
    "# use MnistDataset class to handle the data\n",
    "train_data = ...\n",
    "val_data = ...\n",
    "test_data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee84884",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378af35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels=1, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            ..., # conv1, input_channel -> 4\n",
    "            ..., # relu\n",
    "            ..., # conv2, channel 4 -> 8\n",
    "            ..., # relu\n",
    "            ..., # conv3, channel 8 -> 16\n",
    "            ..., # relu\n",
    "            ..., # conv4, channel 16 -> 32\n",
    "            ..., # relu\n",
    "            ..., # flatten\n",
    "        )\n",
    "        \n",
    "        # manually calculate the dimension after all convolutions\n",
    "        dim_after_conv = ...\n",
    "        hidden_dim = 32 * dim_after_conv * dim_after_conv\n",
    "        \n",
    "        # Readout layer is mu\n",
    "        self.readout_mu = nn.Linear(hidden_dim, z_dim)\n",
    "        # Readout layer\n",
    "        self.readout_sigma = nn.Linear(hidden_dim, z_dim)\n",
    "        \n",
    "        # You can use nn.ConvTranspose2d to decode\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, hidden_dim),\n",
    "            nn.Unflatten(1, (32, dim_after_conv, dim_after_conv)),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1), # transpose-conv, channel 32 -> 16\n",
    "            ..., # relu\n",
    "            ..., # transpose-conv, channel 16 -> 8\n",
    "            ..., # relu\n",
    "            ..., # transpose-conv, channel 8 -> 4\n",
    "            ..., # relu\n",
    "            ..., # transpose-conv, channel 4 -> input_channel, which is 1\n",
    "            ..., # use a sigmoid activation to squeeze the outputs between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, sigma):\n",
    "        \"\"\"\n",
    "        Reparameterize, i.e. generate a z ~ N(\\mu, \\sigma)\n",
    "        \"\"\"\n",
    "        # generate epsilon ~ N(0, I)\n",
    "        # hint: use torch.randn or torch.randn_like\n",
    "        epsilon = ...\n",
    "        # z = \\mu + \\sigma * \\epsilon\n",
    "        z = ...\n",
    "        return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        # call the encoder to map input to a hidden state vector\n",
    "        h = ...\n",
    "        # use the \"readout\" layer to get \\mu and \\sigma\n",
    "        mu = ...\n",
    "        sigma = ...\n",
    "        return mu, sigma\n",
    "\n",
    "    def decode(self, z):\n",
    "        # call the decoder to map z back to x\n",
    "        return ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encode(x)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6afe3",
   "metadata": {},
   "source": [
    "## (c)\n",
    "\n",
    "*For debugging*: The `test_kld_loss_func` should output 1.3863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kld_loss_func(mu, sigma):\n",
    "    \"\"\"\n",
    "    KL-Divergence: KLD = 0.5 * sum(\\mu^2 + \\sigma^2 - ln(\\sigma^2) - 1)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu: torch.Tensor\n",
    "        Mean vector in the VAE bottleneck region\n",
    "    sigma: torch.Tensor\n",
    "        Standard Deviation vector in the VAE bottleneck region\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    kld: torch.Tensor\n",
    "        KL-Divegence loss (a scalar)\n",
    "    \"\"\"\n",
    "    return ...\n",
    "\n",
    "\n",
    "def vae_loss_func(recon_x, x, mu, sigma):\n",
    "\n",
    "    bce_loss = nn.BCELoss(reduction='sum')(recon_x, x)\n",
    "    kld_loss = kld_loss_func(mu, sigma)\n",
    "\n",
    "    return bce_loss + kld_loss\n",
    "\n",
    "\n",
    "def test_kld_loss_func():\n",
    "    mu = torch.tensor([0.5, 0.5, 1.0])\n",
    "    sigma = torch.tensor([1.0, 0.5, 0.5])\n",
    "    print(kld_loss_func(mu, sigma))\n",
    "\n",
    "test_kld_loss_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class VAETrainer:\n",
    "    \n",
    "    def __init__(self, model, learning_rate, batch_size, epoch, l2):\n",
    "        self.model = model\n",
    "        num_params = sum(item.numel() for item in model.parameters())\n",
    "        print(f\"{model.__class__.__name__} - Number of parameters: {num_params}\")\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(...)\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "    \n",
    "    def train(self, train_data, val_data, early_stop=True, verbose=True, draw_curve=True):\n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        \n",
    "        weights = self.model.state_dict()\n",
    "        lowest_val_loss = np.inf\n",
    "\n",
    "        for n in tqdm(range(self.epoch), leave=False):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                batch_importance = 1 / len(train_data)\n",
    "                # call the model\n",
    "                X_batch_recon, mu, sigma = self.model(...)\n",
    "                batch_loss = vae_loss_func(...)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += batch_loss.detach().cpu().item() * batch_importance\n",
    "            \n",
    "            train_loss_list.append(epoch_loss)\n",
    "            \n",
    "            val_loss = self.evaluate(val_data, print_loss=False)\n",
    "            val_loss_list.append(val_loss)\n",
    "            \n",
    "            if early_stop:\n",
    "                if val_loss < lowest_val_loss:\n",
    "                    lowest_val_loss = val_loss\n",
    "                    weights = self.model.state_dict()\n",
    "            \n",
    "        if draw_curve:\n",
    "            x_axis = np.arange(self.epoch)\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "            ax.plot(x_axis, train_loss_list, label=\"Train\")\n",
    "            ax.plot(x_axis, val_loss_list, label=\"Validation\")\n",
    "            ax.set_title(\"Total Loss\")\n",
    "            ax.set_xlabel(\"# Epoch\")\n",
    "        \n",
    "        if early_stop:\n",
    "            self.model.load_state_dict(weights)\n",
    "        \n",
    "        return {\n",
    "            \"train_loss_list\": train_loss_list,\n",
    "            \"val_loss_list\": val_loss_list,\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, data, print_loss=True):\n",
    "        self.model.eval()\n",
    "        loader = DataLoader(data, batch_size=self.batch_size)\n",
    "        total_loss = 0.0\n",
    "        for X_batch, y_batch in loader:\n",
    "            with torch.no_grad():\n",
    "                batch_importance = 1 / len(data)\n",
    "                X_batch_recon, mu, sigma = self.model(...)\n",
    "                batch_loss = vae_loss_func(...)\n",
    "                total_loss += batch_loss.detach().cpu().item() * batch_importance\n",
    "        if print_loss:\n",
    "            print(f\"Total Loss: {total_loss}\")\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7201ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "trainer = ...\n",
    "# train\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d70391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaulate the qualitiy of reconstruction\n",
    "def plot_digits(data, title):\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(6, 6))\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    fig.suptitle(title)\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        im = ax.imshow(data[i].reshape(32, 32), cmap='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "def compare_reconstruct(model, X):\n",
    "    plot_digits(X, \"Original Data\")\n",
    "    with torch.no_grad():\n",
    "        X_recon, _, _ = model(X)\n",
    "    plot_digits(X_recon, \"Reconstructed Data\")\n",
    "\n",
    "\n",
    "compare_reconstruct(trainer.model, X_test[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b61c5",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce714b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader as GraphDataLoader\n",
    "from torch_geometric.utils import scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054133cf",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qm9(path=\"./QM9\"):\n",
    "    def transform(data):\n",
    "        edge_index = torch.tensor(\n",
    "            list(itertools.permutations(range(data.x.shape[0]), 2)), \n",
    "            dtype=torch.long\n",
    "        ).T\n",
    "        edge_feature = 1 / torch.sqrt(\n",
    "            torch.sum(\n",
    "                (data.pos[edge_index[0]] - data.pos[edge_index[1]]) ** 2, \n",
    "                axis=1, keepdim=True\n",
    "            )\n",
    "        )\n",
    "        data.edge_index = edge_index\n",
    "        data.edge_attr = edge_feature\n",
    "        data.y = data.y[:, [-7]]\n",
    "        return data\n",
    "    \n",
    "    qm9 = QM9(path, transform=transform)\n",
    "    return qm9\n",
    "\n",
    "qm9 = load_qm9(...)\n",
    "train_index, test_index = ... # use train_test_split to do the index\n",
    "train_data = ...\n",
    "test_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the dimension of node input features\n",
    "node_input_dim = ...\n",
    "edge_input_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f2e9d",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927515f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic layer, a linear layer with a ReLU activation \n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            ..., # linear layer\n",
    "            ... # relu\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class MessagePassingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A message passing layer that updates nodes/edge features\n",
    "    \"\"\"\n",
    "    def __init__(self, node_hidden_dim, edge_hidden_dim):\n",
    "        super().__init__()\n",
    "        # figure out the input/output dimension\n",
    "        self.edge_net = Layer(...)\n",
    "        # figure out the input/output dimension\n",
    "        self.node_net = Layer(...)\n",
    "    \n",
    "    def forward(self, node_features, edge_features, edge_index):\n",
    "        \"\"\"\n",
    "        Update node and edge features\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node_features: torch.Tensor\n",
    "            Node features from the previous layer\n",
    "        edge_features: torch.Tensor\n",
    "            Edge features from the previous layer\n",
    "        edge_index: torch.Tensor\n",
    "            A sparse matrix (n_edge, 2) in which each column denotes node indices forming an edge\n",
    "        \"\"\"\n",
    "        # concatnate previous edge features with node features forming the edge\n",
    "        # hint: use node_features[edge_index[0(or 1)]] to get node features forming the edge\n",
    "        concate_edge_features = torch.cat([\n",
    "            ..., # features of one node\n",
    "            ..., # features of the other node\n",
    "            ... # previous edge features\n",
    "        ], dim=1)\n",
    "        \n",
    "        # pass through the \"edge_net\" to map it back to the original dimension\n",
    "        updated_edge_features = self.edge_net(...)\n",
    "        \n",
    "        \n",
    "        # use scatter to aggrate the edge features to nodes\n",
    "        aggr_edge_features = scatter(...)\n",
    "        # concatenate it with previous node features\n",
    "        concate_node_features = torch.cat([..., ...], dim=1)\n",
    "        # pass through the \"node_net\" to map it back to the original dimension\n",
    "        updated_node_features = self.node_net(...)\n",
    "        \n",
    "        return updated_node_features, updated_edge_features\n",
    "\n",
    "        \n",
    "class GraphNet(nn.Module):\n",
    "    def __init__(self, node_input_dim, edge_input_dim, node_hidden_dim, edge_hidden_dim):\n",
    "        super().__init__()\n",
    "        # embed the input node features\n",
    "        self.node_embed = Layer(...)\n",
    "        # embed the input edge features\n",
    "        self.edge_embed = Layer(...)\n",
    "        # use a linear layer as readout to get the \"atomic\" energy contribution\n",
    "        self.readout = ...\n",
    "        # message passing layer\n",
    "        self.message_passing = MessagePassingLayer(..., ...)\n",
    "    \n",
    "    def forward(self, node_features, edge_features, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Update node and edge features\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node_features: torch.Tensor\n",
    "            Node features from the previous layer\n",
    "        edge_features: torch.Tensor\n",
    "            Edge features from the previous layer\n",
    "        edge_index: torch.Tensor\n",
    "            A sparse matrix (n_edges, 2) in which each column denotes node indices forming an edge\n",
    "        batch: torch.Tensor\n",
    "            A 1-D tensor (n_nodes,) that tells you each node belongs to which graph\n",
    "        \"\"\"\n",
    "        node_hidden = ... # call the node embedding\n",
    "        edge_hidden = ... # call the edge embedding\n",
    "        updated_node_hidden, updated_edge_hidden = ... # call the message passing layer\n",
    "        readout = ... # use the readout layer to output \"atomic\" contributions\n",
    "        out = ... # use the scatter function to aggregate atomic readouts\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNTrainer:\n",
    "    def __init__(self, model, batch_size, learning_rate, epoch, l2):\n",
    "        self.model = model\n",
    "        \n",
    "        num_params = sum(item.numel() for item in model.parameters())\n",
    "        print(f\"{model.__class__.__name__} - Number of parameters: {num_params}\")\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = torch.optim.Adam(...)\n",
    "        self.epoch = epoch\n",
    "    \n",
    "    def train(self, dataset, draw_curve=True):\n",
    "        self.model.train()\n",
    "        loader = GraphDataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        loss_func = nn.MSELoss()\n",
    "        batch_loss_list = []\n",
    "        for i in range(self.epoch):\n",
    "            print(f\"Epoch: {i}\")\n",
    "            for batch_data in tqdm(loader, leave=False):\n",
    "                batch_size = batch_data.y.shape[0]\n",
    "                batch_pred = self.model(...)\n",
    "                batch_loss = loss_func(...)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                batch_loss_list.append(batch_loss.detach().numpy())\n",
    "        \n",
    "        if draw_curve:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(5, 4), constrained_layout=True)\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.plot(np.arange(len(batch_loss_list)), batch_loss_list)\n",
    "            ax.set_xlabel(\"# Batch\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "        \n",
    "        return batch_loss_list\n",
    "    \n",
    "    def evaluate(self, dataset, draw_curve=True):\n",
    "        self.model.eval()\n",
    "        loader = GraphDataLoader(dataset, batch_size=self.batch_size)\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_data in tqdm(loader, leave=False):\n",
    "                batch_pred = self.model(...)\n",
    "                y_pred.append(batch_pred.detach().numpy().flatten())\n",
    "                y_true.append(batch_data.y.detach().numpy().flatten())\n",
    "        \n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        mse = np.mean((y_true - y_pred) ** 2)\n",
    "        \n",
    "        if draw_curve:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(5, 4), constrained_layout=True)\n",
    "            ax.scatter(y_true, y_pred, label=f\"MSE: {mse:.2f}\", s=2)\n",
    "            ax.set_xlabel(\"Ground Truth\")\n",
    "            ax.set_ylabel(\"Predicted\")\n",
    "            xmin, xmax = ax.get_xlim()\n",
    "            ymin, ymax = ax.get_ylim()\n",
    "            vmin, vmax = min(xmin, ymin), max(xmax, ymax)\n",
    "            ax.set_xlim(vmin, vmax)\n",
    "            ax.set_ylim(vmin, vmax)\n",
    "            ax.plot([vmin, vmax], [vmin, vmax], color='red')\n",
    "            ax.legend()\n",
    "            \n",
    "        return mse\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547af0b",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_hidden_dim = 64\n",
    "edge_hidden_dim = 64\n",
    "\n",
    "net = GraphNet(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d743009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "learning_rate = ...\n",
    "n_epoch = ...\n",
    "batch_size = ...\n",
    "\n",
    "trainer = GNNTrainer(...)\n",
    "trainer.train(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f778c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaulate\n",
    "trainer.evaluate(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c142",
   "language": "python",
   "name": "c142"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
