{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "## 2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "class NaiveBayesClassifier():\n",
    "    def __init__(self):\n",
    "        self.type_indices={}    # store the indices of wines that belong to each cultivar as a boolean array of length 178\n",
    "        self.type_stats={}      # store the mean and std of each cultivar\n",
    "        self.ndata = 0\n",
    "        self.trained=False\n",
    "    \n",
    "    @staticmethod\n",
    "    def gaussian(x,mean,std):\n",
    "        return ...\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_statistics(x_values):\n",
    "        # Returns a list with length of input features. Each element is a tuple, with the input feature's average and standard deviation\n",
    "        n_feats=x_values.shape[1]\n",
    "        return [(np.average(x_values[:,n]),np.std(x_values[:,n])) for n in range(n_feats)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_prob(x_input,stats):\n",
    "        \"\"\"Calculate the probability that the input features belong to a specific class(P(X|C)), defined by the statistics of features in that class\n",
    "        x_input: np.array shape(nfeatures)\n",
    "        stats: list of tuple [(mean1,std1),(means2,std2),...]\n",
    "        \"\"\" \n",
    "        ...\n",
    "        return init_prob\n",
    "    \n",
    "    def fit(self,xs,ys):\n",
    "        # Train the classifier by calculating the statistics of different features in each class\n",
    "        self.ndata = len(ys)\n",
    "        for y in set(ys):\n",
    "            type_filter= (ys==y)\n",
    "            self.type_indices[y]=type_filter\n",
    "            self.type_stats[y]=self.calculate_statistics(xs[type_filter])\n",
    "        self.trained=True\n",
    "            \n",
    "    def predict(self,xs):\n",
    "        # Do the prediction by outputing the class that has highest probability\n",
    "        if len(xs.shape)>1:\n",
    "            print(\"Only accepts one sample at a time!\")\n",
    "        if self.trained:\n",
    "            guess=None\n",
    "            max_prob=0\n",
    "            # P(C|X) = P(X|C)*P(C) / sum_i(P(X|C_i)*P(C_i)) (deniminator for normalization only, can be ignored)\n",
    "            for y_type in self.type_stats:\n",
    "                prob= ...\n",
    "                if prob>max_prob:\n",
    "                    max_prob=prob\n",
    "                    guess=y_type\n",
    "            return guess\n",
    "        else:\n",
    "            print(\"Please train the classifier first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A performance tester:\n",
    "def calculate_accuracy(model,xs,ys):\n",
    "    y_pred=np.zeros_like(ys)\n",
    "    for idx,x in enumerate(xs):\n",
    "        y_pred[idx]=model.predict(x)\n",
    "    return np.sum(ys==y_pred)/len(ys)\n",
    "\n",
    "def KFoldNaiveBayes(k, X, y):\n",
    "    \"\"\"\n",
    "    K-Fold Cross Validation for Naive Bayes Classifier\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    k: int\n",
    "        Number of folds\n",
    "    X: numpy.ndarray\n",
    "        Input data, shape (n_samples, n_features)\n",
    "    y: numpy.ndarray\n",
    "        Class labels, shape (n_samples)\n",
    "    \"\"\"\n",
    "    kf = ...\n",
    "    train_acc_all = []\n",
    "    test_acc_all = []\n",
    "    for train_index, test_index in ...:\n",
    "        X_train, X_test = ...\n",
    "        y_train, y_test = ...\n",
    "        \n",
    "        model = ...\n",
    "        \n",
    "        # Report prediction accuracy for this fold\n",
    "        # use the calculate_accuracy() function\n",
    "        train_acc = ...\n",
    "        train_acc_all.append(train_acc)\n",
    "        test_acc = ...\n",
    "        test_acc_all.append(test_acc)\n",
    "        print(\"Train accuracy:\", train_acc)\n",
    "        print(\"Test accuracy:\", test_acc)\n",
    "    \n",
    "    # report mean & std for the training/testing accuracy\n",
    "    print(\"Final results:\")\n",
    "    print(\"Training accuracy:\", ...)\n",
    "    print(\"Testing  accuracy:\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: cross entropy loss of pytorch (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) works by taking in y_hat(your model prediction) as a tensor of size (Batchsize, number of class) and y(reference values) as a tensor of size(Batchsize,). You can think of each element of the y_hat tensor(X_ij) as the probability that the element i belong to class j. And each element of y should be a int/torch.long that is a number between \\[0,number_of_class-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save a model and reload it by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.state_dict()\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b\n",
    "\n",
    "*For debugging*: The accuracy could reach over 95\\% if the hyperparamters are tuned properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "\n",
    "# you can use this framework to do training and validation           \n",
    "def train_and_val(model,train_X,train_y,epochs,draw_curve=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------\n",
    "    model: a PyTorch model\n",
    "    train_X: np.array shape(ndata,nfeatures)\n",
    "    train_y: np.array shape(ndata)\n",
    "    epochs: int\n",
    "    draw_curve: bool\n",
    "    \"\"\"\n",
    "    ### Define your loss function, optimizer. Convert data to torch tensor ###\n",
    "    optimizer = ...\n",
    "    loss_func = ...\n",
    "    train_X = torch.tensor(train_X, dtype=torch.float)\n",
    "    train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "    \n",
    "    ### Split training examples further into training and validation ###\n",
    "    X_train, X_val, y_train, y_val = ...\n",
    "\n",
    "    val_array=[]\n",
    "    lowest_val_loss = np.inf\n",
    "    model_param = model.state_dict()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        ### Compute the loss and do backpropagation ###\n",
    "        y_pred = model(X_train)\n",
    "        loss = loss_func(y_pred, y_train-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### compute validation loss and keep track of the lowest val loss ###\n",
    "        with torch.no_grad():\n",
    "            val_loss = ...\n",
    "            val_array.append(val_loss)\n",
    "            \n",
    "            if val_loss < lowest_val_loss:\n",
    "                lowest_val_loss = val_loss\n",
    "                model_param = model.state_dict()\n",
    "                \n",
    "     # The final number of epochs is when the minimum error in validation set occurs    \n",
    "    final_epochs=np.argmin(val_array)+1\n",
    "    print(\"Number of epochs with lowest validation:\",final_epochs)\n",
    "    ### Recover the model weight ###\n",
    "    model.load_state_dict(model_param)\n",
    "\n",
    "    if draw_curve:\n",
    "        plt.plot(np.arange(len(val_array))+1,val_array,label='Validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_nn(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        y_pred = torch.argmax(model(X), axis=1)\n",
    "        acc = torch.sum(y_pred == y) / len(y)\n",
    "    return acc.detach().numpy()\n",
    "\n",
    "def KFoldNN(k, X, y, epochs=500):\n",
    "    \"\"\"\n",
    "    K-Fold Validation for Neural Network\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    k: int\n",
    "        Number of folds\n",
    "    X: numpy.ndarray\n",
    "        Input data, shape (n_samples, n_features)\n",
    "    y: numpy.ndarray\n",
    "        Class labels, shape (n_samples)\n",
    "    epochs: int\n",
    "        Number of epochs during training\n",
    "    \"\"\"\n",
    "    # K-Fold\n",
    "    kf = ...\n",
    "    train_acc_all = []\n",
    "    test_acc_all = []\n",
    "    for train_index, test_index in ...:\n",
    "        X_train, X_test = ...\n",
    "        y_train, y_test = ...\n",
    "        \n",
    "        # further do a train/valid split on X_train\n",
    "        model = ...\n",
    "        \n",
    "        # Report prediction accuracy for this fold\n",
    "        # use calculate_accuracy_nn() function\n",
    "        train_acc = ...\n",
    "        train_acc_all.append(train_acc)\n",
    "        test_acc = ...\n",
    "        test_acc_all.append(test_acc)\n",
    "        print(\"Train accuracy:\", train_acc)\n",
    "        print(\"Test accuracy:\", test_acc)\n",
    "    \n",
    "    # report mean & std for the training/testing accuracy\n",
    "    print(\"Final results:\")\n",
    "    print(\"Training accuracy:\", ...)\n",
    "    print(\"Testing  accuracy:\", ...)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
