{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (a) new (x,y) position: [0.15 0.9 ]\n",
    "\n",
    "   (b) Converge after 41 accepted steps. Converge to point [-0.99999982  0.99999455] with value -2.9999999999721534.  took: 0.0020 sec \n",
    "\n",
    "2. (b) It is a stochastic method so your answer may vary. It takes ~1700 steps to converge and took ~0.1 sec\n",
    "\n",
    "3. (b) takes ~250 steps to converge and took ~0.02 sec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "A timing decorator. Put at the beginning of your function so that every time your function is called it'll print out the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def timeit(f):\n",
    "\n",
    "    def timed(*args, **kw):\n",
    "\n",
    "        ts = time.time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time.time()\n",
    "\n",
    "        print(f'func:{f.__name__} took: {te-ts:.4f} sec')\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that help to visualize the optimization pathway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def draw_path(func,path,x_min=-2,x_max=2,y_min=-2,y_max=2):\n",
    "    a=np.linspace(x_min,x_max,100)\n",
    "    b=np.linspace(y_min,y_max,100)\n",
    "    x,y=np.meshgrid(a,b)\n",
    "    z=func((x,y))\n",
    "    fig,ax=plt.subplots()\n",
    "    my_contour=ax.contour(x,y,z,50)\n",
    "    plt.colorbar(my_contour)\n",
    "    ax.plot(path[:,0],path[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Templates for algorithm you need to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15, 0.9 ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7.5625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.9419937500000004"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def func2d(X):\n",
    "    '''\n",
    "    A 2D function\n",
    "    '''\n",
    "    x,y = X\n",
    "    return x**4-x**2+y**2+2*x*y-2\n",
    "\n",
    "# def func2d_derivative(X):\n",
    "#     x,y = X\n",
    "#     return (4*x**3) - (2*x) + (2*y)\n",
    "\n",
    "def func2d_derivative(X):\n",
    "    x, y = X\n",
    "    deriv_x = (4*x**3) - (2*x) + (2*y)\n",
    "    deriv_y = (2*y) + (2*x)\n",
    "    return np.array([deriv_x, deriv_y])\n",
    "\n",
    "starting_point = np.array([1.5, 1.5])\n",
    "stepsize = .1\n",
    "new_point = starting_point - stepsize * func2d_derivative(starting_point)\n",
    "display(new_point)\n",
    "display(func2d(starting_point))\n",
    "display(func2d(new_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "As shown above, the new (x,y) position will be (.15, .9). The value of the function at the starting point is 7.5625, while the value of the function at the new point is -0.9419937500000004. Since f(new_point) < f(starting_point), this is considered a good step. Since it is a goodstep, we will change the stepsize for the next step to be 1.2 * stepsize, or $1.2 * \\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def steepest_descent(func,first_derivate,starting_point,stepsize,tol):\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv = first_derivate(starting_point)\n",
    "    \n",
    "    count=0\n",
    "    visited=[]\n",
    "    while np.linalg.norm(deriv) > tol and count < 1e6:\n",
    "        # calculate new point position\n",
    "        deriv = first_derivate(starting_point)\n",
    "        new_point = starting_point - stepsize * deriv\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            stepsize  = 1.2 * stepsize\n",
    "            visited.append(new_point)\n",
    "            starting_point = new_point\n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            stepsize = 0.5 * stepsize\n",
    "        count+=1\n",
    "    # return the results\n",
    "    return {\"x\":starting_point,\"evaluation\":func(starting_point),\"path\":np.asarray(visited)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:steepest_descent took: 0.0018 sec\n",
      "{'x': array([-0.99999972,  0.99999691]), 'evaluation': -2.999999999991799, 'path': array([[-0.03162   ,  0.648     ],\n",
      "       [-0.22733235,  0.47048256],\n",
      "       [-0.4603766 ,  0.38644985],\n",
      "       [-0.73063964,  0.41710875],\n",
      "       [-0.91361447,  0.57314178],\n",
      "       [-0.89067253,  0.77647099],\n",
      "       [-0.98168777,  0.81739147],\n",
      "       [-0.94167921,  0.88803587],\n",
      "       [-1.0240441 ,  0.91571465],\n",
      "       [-0.95964931,  0.94925202],\n",
      "       [-1.01216804,  0.95311466],\n",
      "       [-0.98795692,  0.96627781],\n",
      "       [-0.99481157,  0.9720766 ],\n",
      "       [-0.99412388,  0.97937406],\n",
      "       [-0.99741632,  0.98505533],\n",
      "       [-0.99646125,  0.99076871],\n",
      "       [-1.00111334,  0.9939261 ],\n",
      "       [-0.99723697,  0.99631795],\n",
      "       [-1.00126535,  0.99668496],\n",
      "       [-0.99895278,  0.99778247],\n",
      "       [-0.99981882,  0.99811897],\n",
      "       [-0.99948229,  0.99870549],\n",
      "       [-1.00001741,  0.99902712],\n",
      "       [-0.99975409,  0.99927314],\n",
      "       [-0.99990384,  0.99941651],\n",
      "       [-0.99986709,  0.99959085],\n",
      "       [-0.99997668,  0.99970943],\n",
      "       [-0.99988705,  0.9998471 ],\n",
      "       [-1.00001432,  0.99985945],\n",
      "       [-0.99993563,  0.99991689],\n",
      "       [-0.99998875,  0.99992106],\n",
      "       [-0.99998269,  0.99993914],\n",
      "       [-0.99999092,  0.9999531 ],\n",
      "       [-0.99999034,  0.99996764],\n",
      "       [-0.9999977 ,  0.99997812],\n",
      "       [-0.99999196,  0.99998896],\n",
      "       [-1.00000165,  0.99998995],\n",
      "       [-0.99999435,  0.99999462],\n",
      "       [-0.99999982,  0.99999455],\n",
      "       [-0.99999852,  0.99999607],\n",
      "       [-0.99999972,  0.99999691]])}\n"
     ]
    }
   ],
   "source": [
    "# result = steepest_descent(func2d, \n",
    "#                  func2d_derivative, \n",
    "#                  starting_point=np.array([1.5, 1.5]), \n",
    "#                  stepsize=.1, \n",
    "#                  tol=1e-5)\n",
    "result = steepest_descent(func2d,\n",
    "                         func2d_derivative,\n",
    "                          starting_point = new_point,\n",
    "                          stepsize = .1 * 1.2,\n",
    "                          tol = 1E-5\n",
    "                         )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(result['path']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer \n",
    "As shown above, continuing steepest descents takes 41 accepted steps in order to converge to the local minimum with tolerance = $ 1 * 10^{-5} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def save_step(*args):\n",
    "    for arg in args:\n",
    "        if type(arg) is np.ndarray:\n",
    "            steps.append(arg)\n",
    "\n",
    "@timeit\n",
    "def minimize_function(x0, func, method):\n",
    "    \"\"\"\n",
    "    Minimize a function\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    x0: np.ndarray\n",
    "        Starting point\n",
    "    func: function\n",
    "        Scalar function to minimize\n",
    "    method: str\n",
    "        Method for minimization\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    res: OptimzizeResult\n",
    "        Result object of scipy optimization\n",
    "    \"\"\"\n",
    "    res = minimize(\n",
    "        func,\n",
    "        x0,\n",
    "        method=method,\n",
    "        options={\"gtol\": 1e-5, \"disp\":True},\n",
    "        callback=save_step,\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -3.000000\n",
      "         Iterations: 9\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 26\n",
      "func:minimize_function took: 0.0153 sec\n",
      "[-0.99999984  0.99999929]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Conjugate Gradients minimization\n",
    "\n",
    "steps = [starting_point]\n",
    "res = minimize_function(starting_point, func2d, \"CG\")\n",
    "print(res.x)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -3.000000\n",
      "         Iterations: 7\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 8\n",
      "func:minimize_function took: 0.0067 sec\n",
      "[ 0.99999979 -0.9999998 ]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# BFGS Minimization\n",
    "\n",
    "steps = [starting_point]\n",
    "res = minimize_function(starting_point, func2d, \"BFGS\")\n",
    "print(res.x)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer \n",
    "\n",
    "In terms of steps, both conjugate gradient and BFGS minimization are more efficient than steepest descent. CG only took 10 steps, while BFGS only took 8 steps (including the initial starting step). Meanwhile, steepest descent took 41 steps, so both CG and BFGS are more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:steepest_descent took: 0.0242 sec\n",
      "{'x': array([0.99999105, 0.99998163]), 'evaluation': 8.230174326047824e-11, 'path': array([[-1.05      ,  0.875     ],\n",
      "       [-0.845175  ,  0.94325   ],\n",
      "       [-0.91805808,  0.86083548],\n",
      "       ...,\n",
      "       [ 0.99999093,  0.99998135],\n",
      "       [ 0.99999089,  0.99998153],\n",
      "       [ 0.99999105,  0.99998163]])}\n"
     ]
    }
   ],
   "source": [
    "def rosenbrock(X):\n",
    "    x, y = X\n",
    "    return (1 - x)**2 + 10 * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_derivative(X):\n",
    "    x, y = X\n",
    "    partial_x = 2 * (x - 1) - 40 * x * (y - x**2)\n",
    "    partial_y = 20 * (y - x ** 2)\n",
    "    return np.array([partial_x, partial_y])\n",
    "\n",
    "q2_start = np.array([-.5, 1.5])\n",
    "result = steepest_descent(rosenbrock, rosenbrock_derivative, starting_point = q2_start, stepsize = 0.1, tol = 1e-5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1205\n"
     ]
    }
   ],
   "source": [
    "print(len(result['path']))\n",
    "# print(np.linalg.norm(rosenbrock_derivative(result['path'][-2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "Convergence to the minimum of the Rosenbrock function using my steepest descent algorithm took 1205 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deriv = rosenbrock_derivative(q2_start)\n",
    "random_array = rosenbrock_derivative(np.random.rand(deriv.shape[0]))\n",
    "stochastic_deriv = random_array * (np.linalg.norm(deriv) / np.linalg.norm(random_array))\n",
    "np.linalg.norm(deriv) == np.linalg.norm(stochastic_deriv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def stochastic_gradient_descent(func,first_derivate,starting_point,stepsize,tol=1e-5,stochastic_injection=1):\n",
    "    '''stochastic_injection: controls the magnitude of stochasticity (multiplied with stochastic_deriv)\n",
    "        0 for no stochasticity, equivalent to SD. \n",
    "        Use 1 in this homework to run SGD\n",
    "    '''\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv = first_derivate(starting_point)\n",
    "    count=0\n",
    "    visited=[]\n",
    "    while np.linalg.norm(deriv) > tol and count < 1e5:\n",
    "        deriv = first_derivate(starting_point)\n",
    "        if stochastic_injection>0:\n",
    "            stochastic_deriv = np.random.randn(deriv.shape[0])\n",
    "            stochastic_deriv *= (np.linalg.norm(deriv) / np.linalg.norm(stochastic_deriv))\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv)\n",
    "        # calculate new point position\n",
    "        new_point = starting_point + (direction * stepsize)\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            stepsize  = 1.2 * stepsize\n",
    "            visited.append(new_point)\n",
    "            starting_point = new_point\n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            stepsize = 0.5 * stepsize\n",
    "        count+=1\n",
    "    return {\"x\":starting_point,\"evaluation\":func(starting_point),\"path\":np.asarray(visited), 'count':count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:stochastic_gradient_descent took: 0.0440 sec\n",
      "{'x': array([0.9999893 , 0.99997822]), 'evaluation': 1.1588020850588699e-10, 'path': array([[-0.87063551,  1.54699991],\n",
      "       [-1.38606491,  1.61024359],\n",
      "       [-1.31603513,  1.79393191],\n",
      "       ...,\n",
      "       [ 0.99998932,  0.99997803],\n",
      "       [ 0.99998924,  0.99997803],\n",
      "       [ 0.9999893 ,  0.99997822]]), 'count': 2258}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1788"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = stochastic_gradient_descent(rosenbrock, rosenbrock_derivative, np.array([-0.5, 1.5]), stepsize=.01, tol=1e-5, stochastic_injection=1)\n",
    "print(result)\n",
    "len(result['path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 20\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 44\n",
      "func:minimize_function took: 0.0179 sec\n",
      "[0.99999955 0.99999908]\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "# Conjugate Gradients minimization Rosenbrock\n",
    "\n",
    "steps = [q2_start]\n",
    "res = minimize_function(q2_start, rosenbrock, \"CG\")\n",
    "print(res.x)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 22\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 31\n",
      "func:minimize_function took: 0.0133 sec\n",
      "[0.99999959 0.99999917]\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# BFGS Minimization Rosenbrock\n",
    "\n",
    "steps = [q2_start]\n",
    "res = minimize_function(q2_start, rosenbrock, \"BFGS\")\n",
    "print(res.x)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGD optimization algorithm is significantly worse than the Conjugate Gradients or BFGS algorithm, as CG and BFGS only take 20 and 22 iterations respectively, while the SGD takes ~1700 steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 d)\n",
    "\n",
    "You can draw a firm conclusion with just one run of CG and BFGS because each run of those algorithms will always have the exact same output; this is because these algorithms aren't stochastic, and will follow the same optimization path each time. However, for our stochastic gradient descent, because we are adding in randomization, we need to evaluate it multiple times; this is because each time we run it, the optimization path will be different due to the addition of randomness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 13\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 37\n",
      "func:minimize_function took: 0.0149 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 48\n",
      "         Function evaluations: 177\n",
      "         Gradient evaluations: 59\n",
      "func:minimize_function took: 0.0124 sec\n",
      "func:stochastic_gradient_descent took: 0.0289 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 14\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 34\n",
      "func:minimize_function took: 0.0026 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 24\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 30\n",
      "func:minimize_function took: 0.0033 sec\n",
      "func:stochastic_gradient_descent took: 0.0229 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 21\n",
      "         Function evaluations: 171\n",
      "         Gradient evaluations: 57\n",
      "func:minimize_function took: 0.0040 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 61\n",
      "         Function evaluations: 240\n",
      "         Gradient evaluations: 80\n",
      "func:minimize_function took: 0.0063 sec\n",
      "func:stochastic_gradient_descent took: 0.0193 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 12\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 32\n",
      "func:minimize_function took: 0.0022 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 53\n",
      "         Function evaluations: 192\n",
      "         Gradient evaluations: 64\n",
      "func:minimize_function took: 0.0047 sec\n",
      "func:stochastic_gradient_descent took: 0.0218 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 9\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 24\n",
      "func:minimize_function took: 0.0018 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 30\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 39\n",
      "func:minimize_function took: 0.0032 sec\n",
      "func:stochastic_gradient_descent took: 0.0204 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 17\n",
      "         Function evaluations: 141\n",
      "         Gradient evaluations: 47\n",
      "func:minimize_function took: 0.0032 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 22\n",
      "         Function evaluations: 84\n",
      "         Gradient evaluations: 28\n",
      "func:minimize_function took: 0.0021 sec\n",
      "func:stochastic_gradient_descent took: 0.0203 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 16\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 41\n",
      "func:minimize_function took: 0.0029 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 61\n",
      "         Function evaluations: 231\n",
      "         Gradient evaluations: 77\n",
      "func:minimize_function took: 0.0054 sec\n",
      "func:stochastic_gradient_descent took: 0.0221 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 15\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 39\n",
      "func:minimize_function took: 0.0027 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 55\n",
      "         Function evaluations: 222\n",
      "         Gradient evaluations: 74\n",
      "func:minimize_function took: 0.0052 sec\n",
      "func:stochastic_gradient_descent took: 0.0220 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 18\n",
      "         Function evaluations: 147\n",
      "         Gradient evaluations: 49\n",
      "func:minimize_function took: 0.0038 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 59\n",
      "         Function evaluations: 207\n",
      "         Gradient evaluations: 69\n",
      "func:minimize_function took: 0.0051 sec\n",
      "func:stochastic_gradient_descent took: 0.0202 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 15\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 34\n",
      "func:minimize_function took: 0.0022 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 46\n",
      "         Function evaluations: 171\n",
      "         Gradient evaluations: 57\n",
      "func:minimize_function took: 0.0041 sec\n",
      "func:stochastic_gradient_descent took: 0.0190 sec\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "averageCG, averageBFGS, averageSGD = 0, 0, 0\n",
    "numiters = 10\n",
    "for i in range(numiters):\n",
    "    start = random.sample(range(-20, 20), 2)\n",
    "    steps = []\n",
    "    minimize_function(start, rosenbrock, \"CG\")\n",
    "    averageCG += len(steps)\n",
    "\n",
    "    steps = []\n",
    "    minimize_function(start, rosenbrock, \"BFGS\")\n",
    "    averageBFGS += len(steps)\n",
    "\n",
    "    SGD = stochastic_gradient_descent(rosenbrock, \n",
    "                                      rosenbrock_derivative, \n",
    "                                      q2_start, \n",
    "                                      stepsize = .1, \n",
    "                                      tol=1e-5, \n",
    "                                      stochastic_injection=1)\n",
    "    averageSGD = len(SGD['path'])\n",
    "\n",
    "averageCG /= numiters\n",
    "averageBFGS /= numiters\n",
    "averageSGD /= numiters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0 48.2 171.2\n"
     ]
    }
   ],
   "source": [
    "print(averageCG, averageBFGS, averageSGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "For optimization of the Rosenbrock Banana Function, the CG method works best, followed by BFGS, and finally SGD. On average, CG takes the least number of steps, and BFGS follows closely behind. However, SGD takes significantly more steps to converge. Therefore, the non-stochastic methods of CG and BFGS are superior performers for this Rosenbrock Banana Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_hump_camel(X):\n",
    "    x, y = X\n",
    "    return 2 * x**2 - 1.05 * x**4 + (x**6) / 6 + x * y + y**2\n",
    "\n",
    "def three_hump_camel_derivative(X):\n",
    "    x, y = X\n",
    "    partial_x = 4 * x**3 - 4.2 * x**3 + x**5 + y\n",
    "    partial_y = x + 2 * y\n",
    "    return np.array([partial_x, partial_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:stochastic_gradient_descent took: 0.8998 sec\n",
      "[ 0.28104285 -0.09157593]\n",
      "20\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "SGD = stochastic_gradient_descent(three_hump_camel, \n",
    "                                      three_hump_camel_derivative, \n",
    "                                      np.array([.5, .5]), \n",
    "                                      stepsize = .1, \n",
    "                                      tol=1e-5, \n",
    "                                      stochastic_injection=1)\n",
    "print(SGD[\"x\"])\n",
    "print(len(SGD['path']))\n",
    "print(SGD['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 8\n",
      "         Function evaluations: 60\n",
      "         Gradient evaluations: 20\n",
      "func:minimize_function took: 0.0061 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 8\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 10\n",
      "func:minimize_function took: 0.0024 sec\n",
      "func:stochastic_gradient_descent took: 0.8918 sec\n",
      "Stochastic gradient current func value: 0.05595688781942402\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 7\n",
      "         Function evaluations: 57\n",
      "         Gradient evaluations: 19\n",
      "func:minimize_function took: 0.0017 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 8\n",
      "         Function evaluations: 48\n",
      "         Gradient evaluations: 16\n",
      "func:minimize_function took: 0.0015 sec\n",
      "func:stochastic_gradient_descent took: 0.8807 sec\n",
      "Stochastic gradient current func value: 0.7553429642993508\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 7\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 15\n",
      "func:minimize_function took: 0.0013 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 8\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 11\n",
      "func:minimize_function took: 0.0010 sec\n",
      "func:stochastic_gradient_descent took: 0.8743 sec\n",
      "Stochastic gradient current func value: 0.42200959091050694\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 6\n",
      "         Function evaluations: 39\n",
      "         Gradient evaluations: 13\n",
      "func:minimize_function took: 0.0011 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 7\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 10\n",
      "func:minimize_function took: 0.0008 sec\n",
      "func:stochastic_gradient_descent took: 0.8838 sec\n",
      "Stochastic gradient current func value: 0.721553306867756\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 5\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 11\n",
      "func:minimize_function took: 0.0010 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 5\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 8\n",
      "func:minimize_function took: 0.0007 sec\n",
      "func:stochastic_gradient_descent took: 0.8882 sec\n",
      "Stochastic gradient current func value: 0.347297151489511\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 6\n",
      "         Function evaluations: 39\n",
      "         Gradient evaluations: 13\n",
      "func:minimize_function took: 0.0013 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 6\n",
      "         Function evaluations: 27\n",
      "         Gradient evaluations: 9\n",
      "func:minimize_function took: 0.0008 sec\n",
      "func:stochastic_gradient_descent took: 0.8776 sec\n",
      "Stochastic gradient current func value: 0.6733961130555794\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 7\n",
      "         Function evaluations: 39\n",
      "         Gradient evaluations: 13\n",
      "func:minimize_function took: 0.0012 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 7\n",
      "         Function evaluations: 27\n",
      "         Gradient evaluations: 9\n",
      "func:minimize_function took: 0.0009 sec\n",
      "func:stochastic_gradient_descent took: 0.8751 sec\n",
      "Stochastic gradient current func value: 0.9243989916566333\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 5\n",
      "         Function evaluations: 36\n",
      "         Gradient evaluations: 12\n",
      "func:minimize_function took: 0.0011 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 6\n",
      "         Function evaluations: 27\n",
      "         Gradient evaluations: 9\n",
      "func:minimize_function took: 0.0008 sec\n",
      "func:stochastic_gradient_descent took: 0.8892 sec\n",
      "Stochastic gradient current func value: 0.6740915373917729\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 8\n",
      "         Function evaluations: 51\n",
      "         Gradient evaluations: 17\n",
      "func:minimize_function took: 0.0014 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 6\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 8\n",
      "func:minimize_function took: 0.0007 sec\n",
      "func:stochastic_gradient_descent took: 0.8858 sec\n",
      "Stochastic gradient current func value: 0.8314656463620618\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 4\n",
      "         Function evaluations: 27\n",
      "         Gradient evaluations: 9\n",
      "func:minimize_function took: 0.0011 sec\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 5\n",
      "         Function evaluations: 21\n",
      "         Gradient evaluations: 7\n",
      "func:minimize_function took: 0.0006 sec\n",
      "func:stochastic_gradient_descent took: 0.8689 sec\n",
      "Stochastic gradient current func value: 0.10406560929362686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageCG, averageBFGS, averageSGD = 0, 0, 0\n",
    "numFoundGlobalCG, numFoundGlobalBFGS, numFoundGlobalSGD = 0, 0, 0\n",
    "global_min = np.array([0, 0])\n",
    "numiters = 10\n",
    "for i in range(numiters):\n",
    "    random_floats = np.random.uniform(-2, 2, size=2)\n",
    "    start = np.array(random_floats)\n",
    "    steps = []\n",
    "    result = minimize_function(start, three_hump_camel, \"CG\")\n",
    "    averageCG += len(steps)\n",
    "    if np.isclose(a=result.x[1], b=0):\n",
    "        numFoundGlobalCG += 1\n",
    "\n",
    "    steps = []\n",
    "    result = minimize_function(start, three_hump_camel, \"BFGS\")\n",
    "    averageBFGS += len(steps)\n",
    "    if np.isclose(result.x[1], b=0):\n",
    "        numFoundGlobalBFGS += 1\n",
    "\n",
    "    SGD = stochastic_gradient_descent(three_hump_camel, \n",
    "                                      three_hump_camel_derivative, \n",
    "                                      start, \n",
    "                                      stepsize = .1, \n",
    "                                      tol=1e-5, \n",
    "                                      stochastic_injection=1)\n",
    "    averageSGD += len(SGD['path'])\n",
    "    print(f\"Stochastic gradient current func value: {SGD['evaluation']}\")\n",
    "    if np.isclose(SGD[\"evaluation\"], 0):\n",
    "        numFoundGlobalSGD += 1\n",
    "\n",
    "averageCG /= numiters\n",
    "averageBFGS /= numiters\n",
    "averageSGD /= numiters\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3 6.6 59.9\n",
      "2 1 0\n"
     ]
    }
   ],
   "source": [
    "print(averageCG, averageBFGS, averageSGD)\n",
    "print(numFoundGlobalCG, numFoundGlobalBFGS, numFoundGlobalSGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "On average, using the stochastic gradient descent did not take fewer steps when converging to the global minimum, and often times it converged less times than the CG or BFGS algorithms. Likely, the SGD algorithm is getting stuck at a local minimum and thus doesn't converge to the global minimum. Thus, CG and BFGS still outperform SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def SGDM(func,first_derivate,starting_point,stepsize,momentum=0.9,tol=1e-5,stochastic_injection=1):\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv = first_derivate(starting_point)\n",
    "    count=0\n",
    "    visited=[]\n",
    "    previous_direction = np.zeros(len(starting_point))\n",
    "    # previous_point = starting_point\n",
    "    while np.linalg.norm(deriv) > tol and count < 1e5:\n",
    "        deriv = first_derivate(starting_point)\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as your gradient \n",
    "            stochastic_deriv = np.random.randn(deriv.shape[0])\n",
    "            stochastic_deriv *= (np.linalg.norm(deriv) / np.linalg.norm(stochastic_deriv))\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv)\n",
    "        # direction = momentum * previous_direction + (1 - momentum) * direction\n",
    "        # calculate new point position\n",
    "        new_point = starting_point * momentum + (1 - momentum) * direction\n",
    "        if func(new_point) < func(starting_point):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            stepsize = 1.2 * stepsize\n",
    "            visited.append(new_point)\n",
    "            previous_direction = direction\n",
    "            starting_point = new_point\n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            # if stepsize is too small, clear previous direction because we already know that is not a useful direction\n",
    "            if stepsize<1e-5:\n",
    "                previous_direction=previous_direction-previous_direction\n",
    "            else:\n",
    "                # do the same as SGD here\n",
    "                # previous_direction = (direction * stepsize) + (previous_direction * momentum)\n",
    "                stepsize = 0.5 * stepsize\n",
    "                \n",
    "        count+=1\n",
    "    return {\"x\":starting_point,\"evaluation\":func(starting_point),\"path\":np.asarray(visited), 'count':count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:SGDM took: 0.0091 sec\n",
      "{'x': array([-1.94946476e-05,  8.37689943e-06]), 'evaluation': 6.669503108753549e-10, 'path': array([[ 1.09881499e-02,  6.22865336e-02],\n",
      "       [ 5.37132694e-03,  5.73219479e-02],\n",
      "       [-5.22009119e-03,  5.21665480e-02],\n",
      "       [-1.08389313e-02,  2.58764574e-02],\n",
      "       [-7.90453544e-03,  1.72639722e-02],\n",
      "       [-5.79673464e-03,  1.19785179e-02],\n",
      "       [-8.27435071e-03,  7.83533117e-03],\n",
      "       [-8.27735652e-03,  5.23569261e-03],\n",
      "       [-7.85529525e-03,  5.04803335e-03],\n",
      "       [-7.39438044e-03,  3.79706540e-03],\n",
      "       [-7.37741788e-03,  3.23277158e-03],\n",
      "       [-7.00711273e-03,  2.66769674e-03],\n",
      "       [-6.31361723e-03,  2.38992946e-03],\n",
      "       [-5.67314834e-03,  2.44247174e-03],\n",
      "       [-5.60234068e-03,  2.32435116e-03],\n",
      "       [-5.47871314e-03,  2.04086454e-03],\n",
      "       [-5.18702333e-03,  2.21825059e-03],\n",
      "       [-4.80336155e-03,  1.85397282e-03],\n",
      "       [-4.30089045e-03,  1.72065590e-03],\n",
      "       [-4.21552117e-03,  1.54977227e-03],\n",
      "       [-3.76316912e-03,  1.46213514e-03],\n",
      "       [-3.67886686e-03,  1.48441975e-03],\n",
      "       [-3.56670423e-03,  1.28221084e-03],\n",
      "       [-3.50056025e-03,  1.26621168e-03],\n",
      "       [-3.43612521e-03,  1.24760057e-03],\n",
      "       [-3.06114815e-03,  1.21033667e-03],\n",
      "       [-2.84006632e-03,  1.28546890e-03],\n",
      "       [-2.64122991e-03,  1.05987057e-03],\n",
      "       [-2.56687265e-03,  1.08930325e-03],\n",
      "       [-2.49755570e-03,  9.34225328e-04],\n",
      "       [-2.39896103e-03,  1.00041840e-03],\n",
      "       [-2.20624037e-03,  8.46388297e-04],\n",
      "       [-2.03284528e-03,  7.21440496e-04],\n",
      "       [-1.83962936e-03,  7.77805356e-04],\n",
      "       [-1.66176708e-03,  7.69881171e-04],\n",
      "       [-1.64523508e-03,  7.33325930e-04],\n",
      "       [-1.59703407e-03,  7.39887757e-04],\n",
      "       [-1.48245548e-03,  6.08496598e-04],\n",
      "       [-1.45344429e-03,  6.05794201e-04],\n",
      "       [-1.32426123e-03,  5.21631680e-04],\n",
      "       [-1.29763667e-03,  5.22738259e-04],\n",
      "       [-1.16551997e-03,  5.15284178e-04],\n",
      "       [-1.12677061e-03,  4.30915539e-04],\n",
      "       [-1.07868313e-03,  3.68528746e-04],\n",
      "       [-1.00286760e-03,  4.15860189e-04],\n",
      "       [-9.07550962e-04,  3.65281665e-04],\n",
      "       [-8.33881153e-04,  3.10821823e-04],\n",
      "       [-8.17920207e-04,  3.10740567e-04],\n",
      "       [-7.42296798e-04,  2.72269725e-04],\n",
      "       [-7.01657961e-04,  2.31774738e-04],\n",
      "       [-6.24991425e-04,  2.17463036e-04],\n",
      "       [-6.12250902e-04,  2.21756099e-04],\n",
      "       [-5.47196510e-04,  2.26465838e-04],\n",
      "       [-5.39648780e-04,  2.13729989e-04],\n",
      "       [-4.82977076e-04,  2.01893506e-04],\n",
      "       [-4.75972360e-04,  1.84627460e-04],\n",
      "       [-4.27977039e-04,  1.66885168e-04],\n",
      "       [-4.19621469e-04,  1.66832917e-04],\n",
      "       [-3.76665407e-04,  1.52441833e-04],\n",
      "       [-3.45807462e-04,  1.29789797e-04],\n",
      "       [-3.23398030e-04,  1.09872304e-04],\n",
      "       [-3.13411216e-04,  1.19199240e-04],\n",
      "       [-3.07700986e-04,  1.11561413e-04],\n",
      "       [-2.85425739e-04,  9.51185726e-05],\n",
      "       [-2.55065584e-04,  8.78641730e-05],\n",
      "       [-2.27104187e-04,  8.32994379e-05],\n",
      "       [-2.15268410e-04,  9.09960669e-05],\n",
      "       [-2.09470150e-04,  7.81577839e-05],\n",
      "       [-1.86918484e-04,  7.64294764e-05],\n",
      "       [-1.83315451e-04,  6.83748614e-05],\n",
      "       [-1.78826715e-04,  7.05939874e-05],\n",
      "       [-1.60004401e-04,  6.74296423e-05],\n",
      "       [-1.51254500e-04,  5.60225614e-05],\n",
      "       [-1.38441403e-04,  6.03358712e-05],\n",
      "       [-1.34563893e-04,  6.09877080e-05],\n",
      "       [-1.28930818e-04,  6.21315675e-05],\n",
      "       [-1.23622145e-04,  6.24630780e-05],\n",
      "       [-1.12700601e-04,  6.00787638e-05],\n",
      "       [-1.08984482e-04,  5.91784266e-05],\n",
      "       [-9.86678557e-05,  5.50484451e-05],\n",
      "       [-9.21375497e-05,  4.32134306e-05],\n",
      "       [-9.12591934e-05,  4.11624158e-05],\n",
      "       [-9.03681469e-05,  3.70572554e-05],\n",
      "       [-8.53027129e-05,  3.09391174e-05],\n",
      "       [-8.05418600e-05,  2.63662607e-05],\n",
      "       [-7.27759479e-05,  2.95388737e-05],\n",
      "       [-6.85952025e-05,  2.47018970e-05],\n",
      "       [-6.57727389e-05,  2.14434754e-05],\n",
      "       [-6.00646387e-05,  2.44529730e-05],\n",
      "       [-5.78464442e-05,  2.07952260e-05],\n",
      "       [-5.16506515e-05,  1.94674916e-05],\n",
      "       [-4.67361791e-05,  2.03828337e-05],\n",
      "       [-4.33183475e-05,  2.09161312e-05],\n",
      "       [-4.23589654e-05,  2.06333615e-05],\n",
      "       [-4.21694051e-05,  1.80987356e-05],\n",
      "       [-3.97338287e-05,  1.87916969e-05],\n",
      "       [-3.84009422e-05,  1.88590163e-05],\n",
      "       [-3.46337485e-05,  1.75651569e-05],\n",
      "       [-3.20175065e-05,  1.72625923e-05],\n",
      "       [-3.01351400e-05,  1.69818283e-05],\n",
      "       [-2.82359375e-05,  1.65407617e-05],\n",
      "       [-2.80299143e-05,  1.58312678e-05],\n",
      "       [-2.68998171e-05,  1.22630923e-05],\n",
      "       [-2.54579722e-05,  1.25230255e-05],\n",
      "       [-2.40208864e-05,  1.00671902e-05],\n",
      "       [-2.15539421e-05,  9.57663666e-06],\n",
      "       [-1.94946476e-05,  8.37689943e-06]]), 'count': 110}\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "result = SGDM(three_hump_camel, \n",
    "              three_hump_camel_derivative, \n",
    "              starting_point=np.array([0, .1]), \n",
    "              stepsize = .1,\n",
    "              momentum=.9,\n",
    "              tol=1e-5, \n",
    "              stochastic_injection=1)\n",
    "print(result)\n",
    "print(len(result[\"path\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:SGDM took: 0.0110 sec\n",
      "Stochastic gradient momentum current func value: 6.802615626489246e-10\n",
      "func:SGDM took: 0.0033 sec\n",
      "Stochastic gradient momentum current func value: 6.965944793996393e-10\n",
      "func:SGDM took: 0.0030 sec\n",
      "Stochastic gradient momentum current func value: 6.311801876416017e-10\n",
      "func:SGDM took: 0.0031 sec\n",
      "Stochastic gradient momentum current func value: 7.958172354327641e-10\n",
      "func:SGDM took: 0.0022 sec\n",
      "Stochastic gradient momentum current func value: 6.745740366872636e-10\n",
      "func:SGDM took: 0.0027 sec\n",
      "Stochastic gradient momentum current func value: 5.432246923185867e-10\n",
      "func:SGDM took: 0.0028 sec\n",
      "Stochastic gradient momentum current func value: 5.345762873676946e-10\n",
      "func:SGDM took: 0.0025 sec\n",
      "Stochastic gradient momentum current func value: 7.005739930152044e-10\n",
      "func:SGDM took: 0.0022 sec\n",
      "Stochastic gradient momentum current func value: 6.95394778384492e-10\n",
      "func:SGDM took: 0.0022 sec\n",
      "Stochastic gradient momentum current func value: 8.172952479675688e-10\n"
     ]
    }
   ],
   "source": [
    "averageSGDMSteps = 0\n",
    "numFoundGlobalSGDM = 0\n",
    "global_min = np.array([0, 0])\n",
    "numiters = 10\n",
    "for i in range(numiters):\n",
    "    random_floats = np.random.uniform(-2, 2, size=2)\n",
    "    start = np.array(random_floats)\n",
    "\n",
    "    SGDM_results = SGDM(three_hump_camel, \n",
    "                  three_hump_camel_derivative, \n",
    "                  start, \n",
    "                  stepsize = .1, \n",
    "                  momentum = .9,\n",
    "                  tol=1e-5, \n",
    "                  stochastic_injection=1)\n",
    "    \n",
    "    averageSGDMSteps += len(SGDM_results['path'])\n",
    "    print(f\"Stochastic gradient momentum current func value: {SGDM_results['evaluation']}\")\n",
    "    if np.isclose(SGDM_results[\"evaluation\"], 0):\n",
    "        numFoundGlobalSGDM += 1\n",
    "\n",
    "averageSGDMSteps /= numiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161.1\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(averageSGDMSteps)\n",
    "print(numFoundGlobalSGDM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 b)\n",
    "\n",
    "SGDM still takes more steps on average to converge compared to CG or BFGS, but seems to properly find the global minimum more often. While SGD would often get stuck at a local minima, SGDM seems to more accurately find the true global minimum. However, it does often take more steps than CG or BFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
