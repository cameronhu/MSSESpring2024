{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a\n",
    "\n",
    "P[-|M] = 0.05, or the probability that a person has the marker but will test negative equals 5%. This is because this is the complement/opposite of someone having the marker but testing positive. This information was given to us, P[+|M] = 0.95. Together, P[+|M] + P[-|M] = 1, due to both of them being probabilities. Thus, **P[-|M] = 0.05**\n",
    "\n",
    "Similarly, we are given the information P[-|Not M] = 0.95. P[+|Not M] + P[-|Not M] = 1, due to both of them being probabilities and defining the entire population. Therefore, P[+|Not M] = 1 - P[-|Not M] = 0.05. **P[+|Not M] = 0.05**\n",
    "\n",
    "Finally, P[M] = 0.01 is given. Since P[M] + P[Not M] = 1, P[Not M] = 1 - P[M] = .99. **P[Not M] = 0.99**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b\n",
    "\n",
    "Bayes Theorem states that $ P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$. In other words, the probability of A being true given B being true is equal to the probability of B being true given A being true, times the probability of A being true, divided by the probability of B being true.\n",
    "\n",
    "We are asked to find the probability that a person who tested positive (+) actually has the marker (M). Therefore, we are trying to find $ P[M|+] $. Following Bayes' Theorem:\n",
    "\n",
    "$$ P[M|+] = \\frac{P[+|M] * P[M]}{P[+]} $$ \n",
    "\n",
    "We were given the values for P[+|M] and P[M], however we still need to solve for P[+]. We can do this by summing up all the ways to get a positive reading (P[+|M] and P[+|Not M]), multiplyling by their respective probabilities of assumed conditions (P[M] and P[Not M]). Thus,\n",
    "\n",
    "$$ P[+] = P[+|M] * P[M] + P[+|Not M] * P[Not M] $$\n",
    "$$ P[+] = .95 * .01 + .05 * .99 $$\n",
    "$$ P[+] = .059 $$ \n",
    "\n",
    "Finally, we have all our values to solve the original probability, of whether a person who tested positive (+) actually has the marker (M).\n",
    "\n",
    "$$ P[M|+] = \\frac{P[+|M] * P[M]}{P[+]} $$ \n",
    "$$ P[M|+] = \\frac{.95 * .01}{.059} $$ \n",
    "$$ P[M|+] = 0.16101694915 $$ \n",
    "\n",
    "So there is only a 16.1% chance that, after one positive test result, the randomly selected person actually has the marker. Due to the test being relatively inaccurate (having a 5% chance of giving a false positive), along with the probability of actually having the marker being very low (only a 1% chance of actually randomly having the marker), these two factors lead to the low chance that after one positive test, the random person actually has the marker. Since the probability of the marker's prevalence rate is factored into Bayes' Theorem more times than the false positive rate (it is multiplied as P(A) and is also utilized in calculating P(+), the marker's low prevalence is the more dominating factor in terms of why the chance is so low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c\n",
    "\n",
    "If the frequency of the marker increases to P[M] = .1, then surely the chance that P[M|+] would increase. We can recalculate using the updated marker frequency and Bayes' Theorem. We would also need to recalculate P[Not M] and P[+], since those are dependent on the P[M] as well.\n",
    "\n",
    "Solving for P[Not M]:\n",
    "$$ P[Not M] = 1 - P[M] $$\n",
    "$$ P[Not M] = 1 - .1 $$\n",
    "$$ P[Not M] = .9 $$\n",
    "\n",
    "Solving for P[+]:\n",
    "$$ P[+] = P[+|M] * P[M] + P[+|Not M] * P[Not M] $$\n",
    "$$ P[+] = .95 * .1 + .05 * .9 $$\n",
    "$$ P[+] =  .14 $$\n",
    "\n",
    "Plugging in the new values into Bayes' Theorem:\n",
    "$$ P[M|+] = \\frac{P[+|M] * P[M]}{P[+]} $$ \n",
    "$$ P[M|+] = \\frac{.95 * .1}{.14} $$ \n",
    "$$ P[M|+] = 0.67857142857 $$ \n",
    "\n",
    "**Now the percentage has increased to 67.86%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "## 2.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the gaussian function is below. I implemented it using the numpy functions for root, pi, exponent, and square because x is likely to be a vector or array due to the large number of features and data points in our wine dataset. We use a Gaussian function to model the probability because we can presume that most variables in the world are normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k8/mg372j_55z30k1z4y_8mb0w00000gn/T/ipykernel_21276/3183466931.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import numpy as np, pandas as pd, matplotlib.pyplot as plt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "class NaiveBayesClassifier():\n",
    "    def __init__(self):\n",
    "        self.type_indices={}    # store the indices of wines that belong to each cultivar as a boolean array of length 178\n",
    "        self.type_stats={}      # store the mean and std of each cultivar\n",
    "        self.ndata = 0\n",
    "        self.trained=False\n",
    "    \n",
    "    @staticmethod\n",
    "    def gaussian(x,mean,std):\n",
    "        \"\"\"\n",
    "        f(x) = 1 / (sigma*root(2pi))*e^(-1/2*((x-mu)/sigma)^2\n",
    "        \"\"\"\n",
    "        return 1 / (std * np.sqrt(2 * np.pi)) * np.exp((-1/2) * ((x - mean) / std)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_statistics(x_values):\n",
    "        # Returns a list with length of input features. Each element is a tuple, with the input feature's average and standard deviation\n",
    "        n_feats=x_values.shape[1]\n",
    "        return [(np.average(x_values[:,n]),np.std(x_values[:,n])) for n in range(n_feats)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_prob(x_input,stats):\n",
    "        \"\"\"Calculate the probability that the input features belong to a specific class(P(X|C)), defined by the statistics of features in that class\n",
    "        x_input: np.array shape(nfeatures)\n",
    "        stats: list of tuple [(mean1,std1),(means2,std2),...]\n",
    "        \"\"\" \n",
    "        init_prob = 1\n",
    "\n",
    "        for i, (mean, std) in enumerate(stats):\n",
    "            this_prob = NaiveBayesClassifier.gaussian(x_input[i], mean, std) # Take the product of all the individual P(x|yk)\n",
    "            init_prob *= this_prob\n",
    "            # print(f\"This prob is {this_prob}, new init_prob is {init_prob}\")\n",
    "        \n",
    "        return init_prob\n",
    "    \n",
    "    def fit(self,xs,ys):\n",
    "        # Train the classifier by calculating the statistics of different features in each class\n",
    "        self.ndata = len(ys)\n",
    "        for y in set(ys):\n",
    "            type_filter = (ys==y)\n",
    "            self.type_indices[y]=type_filter\n",
    "            self.type_stats[y]=self.calculate_statistics(xs[type_filter])\n",
    "        self.trained=True\n",
    "            \n",
    "    def predict(self,xs):\n",
    "        # Do the prediction by outputing the class that has highest probability\n",
    "        if len(xs.shape)>1:\n",
    "            print(\"Only accepts one sample at a time!\")\n",
    "        if self.trained:\n",
    "            guess=None\n",
    "            max_prob=0\n",
    "            # P(C|X) = P(X|C)*P(C) / sum_i(P(X|C_i)*P(C_i)) (deniminator for normalization only, can be ignored)\n",
    "            for y_type in self.type_stats:\n",
    "                prob = self.calculate_prob(xs, self.type_stats[y_type]) * (type_indices[y_type].sum() / self.ndata)\n",
    "                # print(f\"Probability for rank {y_type} is {prob}\")\n",
    "                if prob>max_prob:\n",
    "                    max_prob=prob\n",
    "                    guess=y_type\n",
    "            return guess\n",
    "        else:\n",
    "            print(\"Please train the classifier first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaiveBayesClassifier.gaussian(20, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol %</th>\n",
       "      <th>Malic Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alkalinity</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Phenols.1</th>\n",
       "      <th>Proantho-cyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280 315</th>\n",
       "      <th>Proline</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.12</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.32</td>\n",
       "      <td>16.8</td>\n",
       "      <td>95</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.41</td>\n",
       "      <td>16.0</td>\n",
       "      <td>89</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol %  Malic Acid   Ash  Alkalinity   Mg  Phenols  Flavanoids  \\\n",
       "0      14.23        1.71  2.43        15.6  127      2.8        3.06   \n",
       "1      13.24        2.59  2.87        21.0  118      2.8        2.69   \n",
       "2      14.83        1.64  2.17        14.0   97      2.8        2.98   \n",
       "3      14.12        1.48  2.32        16.8   95      2.2        2.43   \n",
       "4      13.75        1.73  2.41        16.0   89      2.6        2.76   \n",
       "\n",
       "   Phenols.1  Proantho-cyanins  Color intensity   Hue  OD280 315  Proline  \\\n",
       "0       0.28              2.29             5.64  1.04       3.92     1065   \n",
       "1       0.39              1.82             4.32  1.04       2.93      735   \n",
       "2       0.29              1.98             5.20  1.08       2.85     1045   \n",
       "3       0.26              1.57             5.00  1.17       2.82     1280   \n",
       "4       0.29              1.81             5.60  1.15       2.90     1320   \n",
       "\n",
       "   ranking  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wines = pd.read_csv(\"wines.csv\")\n",
    "wines.drop(labels='Start assignment', axis=1, inplace=True)\n",
    "wines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "71\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    ".fit(self, xs, ys) function debugging\n",
    "\"\"\"\n",
    "\n",
    "# typefilter for ys\n",
    "ys = wines['ranking']\n",
    "type_indices = {} # is a dictionary\n",
    "type_stats = {}\n",
    "for y in set(ys):\n",
    "    type_filter = (ys==y)\n",
    "    type_indices[y] = type_filter\n",
    "    # type_stats[y] = NaiveBayesClassifier.calculate_statistics(xs[type_filter])\n",
    "# type_filter is a boolean array corresponding to Trues if the type is the correct y, else false\n",
    "# Stores the filter in the type_indices dictionary, then calculates statistics (mean and std of each feature) for\n",
    "# each of the three types, and stores in type.stats. Filters the xs to only include the correct type with xs[type_filter],\n",
    "# passing in a boolean array to xs to filter it\n",
    "\n",
    "print(type_indices[1].sum())\n",
    "print(type_indices[2].sum())\n",
    "print(type_indices[3].sum())\n",
    "assert(type_indices[1].sum() + type_indices[2].sum() + type_indices[3].sum() == 178)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol %</th>\n",
       "      <th>Malic Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alkalinity</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Phenols.1</th>\n",
       "      <th>Proantho-cyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280 315</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.12</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.32</td>\n",
       "      <td>16.8</td>\n",
       "      <td>95</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.41</td>\n",
       "      <td>16.0</td>\n",
       "      <td>89</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol %  Malic Acid   Ash  Alkalinity   Mg  Phenols  Flavanoids  \\\n",
       "0      14.23        1.71  2.43        15.6  127      2.8        3.06   \n",
       "1      13.24        2.59  2.87        21.0  118      2.8        2.69   \n",
       "2      14.83        1.64  2.17        14.0   97      2.8        2.98   \n",
       "3      14.12        1.48  2.32        16.8   95      2.2        2.43   \n",
       "4      13.75        1.73  2.41        16.0   89      2.6        2.76   \n",
       "\n",
       "   Phenols.1  Proantho-cyanins  Color intensity   Hue  OD280 315  Proline  \n",
       "0       0.28              2.29             5.64  1.04       3.92     1065  \n",
       "1       0.39              1.82             4.32  1.04       2.93      735  \n",
       "2       0.29              1.98             5.20  1.08       2.85     1045  \n",
       "3       0.26              1.57             5.00  1.17       2.82     1280  \n",
       "4       0.29              1.81             5.60  1.15       2.90     1320  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(178,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: ranking, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_features = wines.iloc[:, :-1]\n",
    "wine_features_np = wine_features.to_numpy()\n",
    "display(wine_features.shape)\n",
    "display(wine_features.head())\n",
    "\n",
    "wine_rankings = wines.iloc[:, -1]\n",
    "wine_rankings_np = wine_rankings.to_numpy()\n",
    "display(wine_rankings.shape)\n",
    "wine_rankings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the WineClassifier, fit to the features and rankings numpy array\n",
    "WineClassifier = NaiveBayesClassifier()\n",
    "WineClassifier.fit(xs=wine_features_np, ys=wine_rankings_np)\n",
    "WineClassifier.type_stats\n",
    "display(len(WineClassifier.type_stats[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the WineClassifier for different data points, see if we can get all the different rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(WineClassifier.predict(wine_features.iloc[0, :].to_numpy()))\n",
    "display(WineClassifier.predict(wine_features.iloc[-1, :].to_numpy()))\n",
    "display(WineClassifier.predict(wine_features.iloc[len(wine_features) // 2, :].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43    3\n",
       "Name: ranking, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test for rankings == 3\n",
    "display(wine_rankings[wine_rankings == 3].head(1))\n",
    "display(WineClassifier.predict(wine_features.iloc[43, :].to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the probability that, given a wine is in Cultivar 1, it has an alcohol content of 13% would be equivalent to asking $ P[13\\%|\\text{Cultivar 1}] $. To do this, we would use our PDF, the Gaussian, with our x-value as 13 and our mean and standard deviation as the statistics determined for the mean/std deviation for cultivar 1, across all confirmed cultivar 1 values. The code for it is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23236757865410357"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1_abv_mean, c1_abv_sd = WineClassifier.type_stats[1][0] # Find the mu and sigma for the C1 Alcohol percentages\n",
    "WineClassifier.gaussian(13, c1_abv_mean, c1_abv_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $ P[13\\%|\\text{Cultivar 1}] = 0.23236757865410357$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.51861254, -0.5622498 ,  0.23205254, ...,  0.36217728,\n",
       "         1.84791957,  1.01300893],\n",
       "       [ 0.29570023,  0.22769377,  1.84040254, ...,  0.36217728,\n",
       "         0.44960118, -0.03787401],\n",
       "       [ 2.25977152, -0.62508622, -0.7183361 , ...,  0.53767082,\n",
       "         0.33660575,  0.94931905],\n",
       "       ...,\n",
       "       [ 0.20923168,  0.22769377,  0.01273209, ..., -1.56825176,\n",
       "        -1.40069891,  0.29649784],\n",
       "       [ 1.39508604,  1.58316512,  1.36520822, ..., -1.52437837,\n",
       "        -1.42894777, -0.59516041],\n",
       "       [-0.92721209, -0.54429654, -0.90110314, ...,  0.18668373,\n",
       "         0.78858745, -0.7543851 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
       "        1.065e+03],\n",
       "       [1.324e+01, 2.590e+00, 2.870e+00, ..., 1.040e+00, 2.930e+00,\n",
       "        7.350e+02],\n",
       "       [1.483e+01, 1.640e+00, 2.170e+00, ..., 1.080e+00, 2.850e+00,\n",
       "        1.045e+03],\n",
       "       ...,\n",
       "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "        8.400e+02],\n",
       "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "        5.600e+02],\n",
       "       [1.225e+01, 1.730e+00, 2.120e+00, ..., 1.000e+00, 3.170e+00,\n",
       "        5.100e+02]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "scaler = StandardScaler()\n",
    "wine_features_norm = scaler.fit_transform(wine_features_np)\n",
    "display(wine_features_norm)\n",
    "display(wine_features_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9830508474576272\n",
      "Test accuracy: 1.0\n",
      "Train accuracy: 0.9915966386554622\n",
      "Test accuracy: 0.9491525423728814\n",
      "Train accuracy: 0.9915966386554622\n",
      "Test accuracy: 0.9661016949152542\n",
      "Final results:\n",
      "Training accuracy mean: 0.9887480415895172, std: 0.0040285246043956485\n",
      "Testing  accuracy mean: 0.9717514124293786, std: 0.021139307269909268\n"
     ]
    }
   ],
   "source": [
    "# A performance tester:\n",
    "def calculate_accuracy(model,xs,ys):\n",
    "    y_pred=np.zeros_like(ys)\n",
    "    for idx,x in enumerate(xs):\n",
    "        y_pred[idx]=model.predict(x)\n",
    "    return np.sum(ys==y_pred)/len(ys)\n",
    "\n",
    "def KFoldNaiveBayes(k, X, y):\n",
    "    \"\"\"\n",
    "    K-Fold Cross Validation for Naive Bayes Classifier\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    k: int\n",
    "        Number of folds\n",
    "    X: numpy.ndarray\n",
    "        Input data, shape (n_samples, n_features)\n",
    "    y: numpy.ndarray\n",
    "        Class labels, shape (n_samples)\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=3)\n",
    "    train_acc_all = []\n",
    "    test_acc_all = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model = NaiveBayesClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Report prediction accuracy for this fold\n",
    "        # use the calculate_accuracy() function\n",
    "        train_acc = calculate_accuracy(model, X_train, y_train)\n",
    "        train_acc_all.append(train_acc)\n",
    "        test_acc = calculate_accuracy(model, X_test, y_test)\n",
    "        test_acc_all.append(test_acc)\n",
    "        print(\"Train accuracy:\", train_acc)\n",
    "        print(\"Test accuracy:\", test_acc)\n",
    "    \n",
    "    # report mean & std for the training/testing accuracy\n",
    "    print(\"Final results:\")\n",
    "    print(f\"Training accuracy mean: {np.mean(train_acc_all)}, std: {np.std(train_acc_all)}\")\n",
    "    print(f\"Testing  accuracy mean: {np.mean(test_acc_all)}, std: {np.std(test_acc_all)}\")\n",
    "\n",
    "KFoldNaiveBayes(5, wine_features_norm, wine_rankings_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: cross entropy loss of pytorch (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) works by taking in y_hat(your model prediction) as a tensor of size (Batchsize, number of class) and y(reference values) as a tensor of size(Batchsize,). You can think of each element of the y_hat tensor(X_ij) as the probability that the element i belong to class j. And each element of y should be a int/torch.long that is a number between \\[0,number_of_class-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save a model and reload it by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # create a NN with no hidden layer, just a linear layer plus a Softmax activation function\n",
    "        self.layers = nn.Linear(13, 3, dtype=torch.float32)\n",
    "        # self.layers = nn.Linear(13, 3, dtype=torch.float64)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "    def forward_softmax(self, X):\n",
    "        Softmax = nn.Softmax(dim=1)\n",
    "        return Softmax(self.layers(X))\n",
    "    \n",
    "\n",
    "model = WineNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1029e-01,  1.0260e-01,  7.2270e-01],\n",
       "        [-7.2586e-01,  5.8687e-01,  2.3956e-01],\n",
       "        [ 2.8051e-01, -4.5739e-01, -1.0251e-01],\n",
       "        [ 1.0230e-01, -2.2169e-01, -2.8597e-01],\n",
       "        [-2.8478e-01, -6.1923e-02, -1.4116e-01],\n",
       "        [-2.4039e-01,  1.7564e-01,  1.0009e-01],\n",
       "        [-1.0671e-01,  3.1025e-01,  7.2162e-01],\n",
       "        [-5.8297e-01,  6.0141e-01,  2.6119e-01],\n",
       "        [-7.6119e-01,  5.8831e-01,  3.2545e-01],\n",
       "        [ 2.3678e-01,  1.3614e-02,  1.4725e-01],\n",
       "        [-2.6121e-02, -1.7209e-01,  3.8673e-01],\n",
       "        [-1.0433e-01,  9.2290e-01,  2.5054e-01],\n",
       "        [-4.0119e-01,  4.9359e-01, -4.7397e-01],\n",
       "        [-3.3989e-01,  3.4648e-01, -2.0866e-01],\n",
       "        [-4.9832e-01,  4.0147e-01,  1.8943e-01],\n",
       "        [-3.2379e-02,  3.3601e-01,  1.0996e+00],\n",
       "        [-2.2997e-01, -2.2081e-01,  3.3548e-01],\n",
       "        [-8.2327e-01,  1.5549e-02,  7.9489e-03],\n",
       "        [-2.9901e-01,  6.1062e-01,  5.0214e-01],\n",
       "        [-3.3536e-01,  3.9782e-01,  4.6831e-01],\n",
       "        [ 7.4922e-01,  4.8873e-01, -6.4152e-01],\n",
       "        [-9.8844e-01,  1.1183e+00,  6.8815e-01],\n",
       "        [-1.4163e-01,  1.5072e-01,  9.1534e-01],\n",
       "        [ 6.9334e-01, -4.3413e-01,  8.7065e-02],\n",
       "        [ 3.8436e-01,  1.1642e-01,  1.5216e-01],\n",
       "        [-8.5170e-01,  8.8828e-01,  1.7368e-01],\n",
       "        [-6.9258e-01,  3.9379e-01,  7.4078e-01],\n",
       "        [-2.5956e-01,  6.0804e-01,  2.6791e-01],\n",
       "        [-4.1228e-01,  5.6952e-01,  4.3729e-01],\n",
       "        [-5.7914e-01,  8.1333e-01,  7.1076e-01],\n",
       "        [-1.7039e-01,  2.9496e-01,  1.6918e+00],\n",
       "        [-2.8849e-01,  2.3801e-01,  2.8016e-02],\n",
       "        [-1.3656e+00,  3.2952e-01,  7.1246e-01],\n",
       "        [-4.9335e-03,  3.7557e-01,  6.8430e-01],\n",
       "        [ 1.2856e-01,  4.3657e-01,  8.0690e-01],\n",
       "        [-7.6131e-01, -1.8992e-01,  2.2217e+00],\n",
       "        [-6.3341e-01,  1.1392e+00,  4.0259e-01],\n",
       "        [-5.0615e-01,  1.1879e+00,  6.2764e-01],\n",
       "        [-1.4546e+00,  1.3863e+00,  7.8758e-01],\n",
       "        [-8.5105e-01,  9.6534e-01,  1.2403e+00],\n",
       "        [-5.4621e-01, -6.0678e-01,  7.3121e-01],\n",
       "        [-4.1815e-01,  6.1502e-01,  1.0589e+00],\n",
       "        [-3.6264e-01, -1.3797e-01,  2.7741e-01],\n",
       "        [ 9.1606e-01, -6.6005e-01, -4.7068e-01],\n",
       "        [ 5.4183e-01, -6.7001e-01, -6.9870e-01],\n",
       "        [ 1.2263e+00, -1.1869e+00, -6.0793e-01],\n",
       "        [ 3.1955e-01,  6.1971e-02, -1.9098e-01],\n",
       "        [ 5.5759e-01, -4.5494e-01, -8.8608e-01],\n",
       "        [ 1.3861e+00, -2.0848e+00, -1.3016e+00],\n",
       "        [ 1.2132e+00, -1.3158e+00, -4.5646e-01],\n",
       "        [ 4.1735e-01, -2.8074e-01, -1.0114e-01],\n",
       "        [ 1.1142e+00, -1.8151e+00, -6.3640e-01],\n",
       "        [ 8.4946e-01, -7.6997e-01, -9.3110e-01],\n",
       "        [ 7.3110e-01, -1.2075e+00, -6.2326e-01],\n",
       "        [ 1.0803e+00, -1.3142e+00, -7.0158e-01],\n",
       "        [ 1.3321e+00, -1.4881e+00, -8.0904e-01],\n",
       "        [ 1.0081e+00, -1.7877e+00, -8.4823e-01],\n",
       "        [ 7.7735e-01, -1.1993e+00, -3.9888e-01],\n",
       "        [ 1.4941e-01,  1.6848e-01,  4.3125e-02],\n",
       "        [-2.5904e-01,  2.5968e-01,  7.2894e-01],\n",
       "        [-4.1312e-02,  1.3303e-01,  2.2286e-01],\n",
       "        [-1.2511e-01,  1.8506e-01,  4.4836e-01],\n",
       "        [-3.8283e-01, -4.5892e-01, -1.4280e-01],\n",
       "        [-2.4409e-01,  5.8365e-01,  2.1942e-01],\n",
       "        [-9.2628e-01, -6.8407e-02,  1.6257e-01],\n",
       "        [-1.1905e+00,  1.1516e+00,  4.7334e-01],\n",
       "        [-8.6777e-01,  7.1774e-01,  5.3820e-02],\n",
       "        [-9.5811e-01,  5.2456e-01,  4.3034e-01],\n",
       "        [-1.9038e-01,  9.6591e-02,  1.8196e-01],\n",
       "        [-3.1136e-01,  3.1945e-01, -7.2219e-02],\n",
       "        [ 6.4072e-02,  2.6817e-01,  6.1893e-02],\n",
       "        [-2.2807e-01, -4.5846e-01,  8.7153e-02],\n",
       "        [-7.8373e-02, -2.8691e-01,  3.2971e-01],\n",
       "        [ 4.4861e-02, -1.8535e-01,  1.0306e-01],\n",
       "        [-2.9134e-02,  3.4874e-02,  7.5474e-01],\n",
       "        [-4.2163e-01, -4.7434e-01,  5.3091e-01],\n",
       "        [-2.5444e-01,  3.7136e-01,  2.3880e-02],\n",
       "        [ 3.0186e-01,  8.4869e-02,  4.4675e-01],\n",
       "        [ 1.5710e+00, -3.6120e-01, -3.8663e-01],\n",
       "        [ 6.8944e-01,  9.1385e-02, -3.0221e-01],\n",
       "        [-4.0394e-01,  7.7290e-01, -5.0199e-01],\n",
       "        [-6.6528e-03,  5.4986e-01,  5.5081e-01],\n",
       "        [-1.4038e+00,  9.2253e-01,  1.0344e-01],\n",
       "        [ 1.7982e-01,  1.5168e-01,  1.0677e-01],\n",
       "        [ 5.1749e-01, -1.8229e-01,  7.5268e-02],\n",
       "        [ 5.3170e-01, -2.7711e-01,  1.0510e+00],\n",
       "        [-5.9807e-01,  1.0163e+00,  5.4582e-01],\n",
       "        [-4.9466e-01,  2.6715e-01,  3.7690e-01],\n",
       "        [-1.1698e+00,  1.1979e+00,  4.4765e-01],\n",
       "        [-6.3298e-02, -4.4111e-02,  1.4144e-02],\n",
       "        [ 4.3487e-01, -3.1084e-01,  1.0628e-01],\n",
       "        [-8.9807e-01,  2.5984e-01,  4.6279e-01],\n",
       "        [ 4.7454e-01, -1.7858e-01,  2.8679e-01],\n",
       "        [-3.6589e-01,  6.6492e-01,  7.0977e-01],\n",
       "        [-1.1694e-01,  4.1882e-01,  9.2041e-01],\n",
       "        [ 1.0076e-01,  2.6747e-01,  1.2533e+00],\n",
       "        [-1.3920e+00,  1.0179e+00,  1.0549e+00],\n",
       "        [-6.5022e-01,  7.1965e-01, -3.8963e-01],\n",
       "        [-1.2016e-01,  4.8644e-01,  1.0358e+00],\n",
       "        [ 1.0020e+00, -9.0582e-01, -3.6298e-01],\n",
       "        [-2.1541e-01, -1.9197e-01,  8.0752e-01],\n",
       "        [-2.2736e-01,  6.8573e-01,  1.1015e+00],\n",
       "        [-8.6730e-01,  9.9389e-01,  8.7300e-01],\n",
       "        [ 8.2740e-01, -8.3689e-01, -8.1704e-01],\n",
       "        [ 5.4921e-01, -9.8874e-01, -8.5394e-01],\n",
       "        [ 6.1404e-01, -4.3378e-01, -5.7791e-01],\n",
       "        [ 1.0624e+00, -8.4201e-01, -5.3640e-01],\n",
       "        [ 4.8720e-01, -1.0961e+00, -7.5322e-01],\n",
       "        [ 1.2299e+00, -1.2379e+00, -3.4368e-02],\n",
       "        [ 1.1018e+00, -1.1935e+00, -3.2684e-01],\n",
       "        [ 6.2963e-01, -3.2690e-01,  1.9106e-01],\n",
       "        [ 7.3678e-01, -7.2806e-01, -1.8483e-01],\n",
       "        [ 6.6924e-01, -4.6423e-01, -3.2953e-01],\n",
       "        [ 1.1158e+00, -1.0862e+00, -5.8445e-01],\n",
       "        [ 1.0813e+00, -1.5976e+00, -6.3979e-01],\n",
       "        [ 1.0230e+00, -8.2184e-01, -4.9496e-01],\n",
       "        [ 1.5468e+00, -1.1142e+00, -6.3348e-01],\n",
       "        [-1.0192e+00,  8.5938e-02,  6.5849e-01],\n",
       "        [-4.0644e-02, -3.9052e-01,  1.0968e-03],\n",
       "        [-8.3805e-04,  3.2370e-01,  6.1145e-02],\n",
       "        [-5.6894e-01,  3.1189e-02,  4.7308e-01],\n",
       "        [-4.8858e-01, -4.1574e-01,  2.1523e-01],\n",
       "        [-4.6555e-01,  4.7193e-01, -3.2839e-01],\n",
       "        [-3.0009e-01,  3.7930e-01,  1.1297e-01],\n",
       "        [-3.7930e-01, -1.4020e-02, -5.1493e-02],\n",
       "        [-3.9442e-01,  3.8296e-01,  4.6170e-01],\n",
       "        [-3.1658e-01,  4.1297e-01,  6.8454e-02],\n",
       "        [-2.8402e-01,  2.5826e-01,  4.7736e-01],\n",
       "        [-4.9242e-01,  4.5613e-01,  6.4780e-01],\n",
       "        [-1.4484e-01,  2.8619e-01,  9.4960e-01],\n",
       "        [-4.9816e-02, -3.4141e-01,  2.8755e-01],\n",
       "        [-4.1926e-01, -3.0528e-01,  3.5336e-01],\n",
       "        [-4.0468e-02,  6.3454e-02,  5.1914e-01],\n",
       "        [-4.2829e-01,  1.7915e-01,  8.1664e-01],\n",
       "        [ 1.1685e-01,  8.6790e-02,  6.0048e-01],\n",
       "        [-5.7379e-01,  4.8567e-01, -1.9810e-02],\n",
       "        [ 1.2315e+00, -4.2665e-02, -3.0934e-01],\n",
       "        [-6.6447e-01,  4.8947e-01,  1.2438e-01],\n",
       "        [ 9.7847e-01,  4.7502e-01, -4.4506e-01],\n",
       "        [ 3.2557e-01,  2.2371e-02,  1.2955e+00],\n",
       "        [-1.2565e+00,  1.8059e+00,  1.5880e+00],\n",
       "        [-7.7867e-01,  1.2263e+00,  1.0423e+00],\n",
       "        [ 3.0013e-01,  4.7328e-02, -8.1232e-02],\n",
       "        [-1.0091e+00,  5.3453e-01,  7.4255e-01],\n",
       "        [ 3.9826e-01, -8.0865e-01,  5.1993e-02],\n",
       "        [-4.5619e-01,  4.9499e-01,  1.6280e-01],\n",
       "        [-1.2204e-01,  1.5153e-01,  3.2428e-01],\n",
       "        [-8.8467e-01,  1.0501e+00,  3.8301e-01],\n",
       "        [-4.0186e-01,  4.5333e-01,  3.8614e-01],\n",
       "        [-7.2666e-01,  1.1072e+00,  1.0299e+00],\n",
       "        [ 2.1720e-01,  2.1395e-01,  8.8879e-01],\n",
       "        [ 2.8347e-01, -1.9673e-01,  3.7612e-01],\n",
       "        [-2.5863e-01,  5.0066e-01,  4.6757e-01],\n",
       "        [-1.4618e-01,  6.1717e-01,  8.6551e-01],\n",
       "        [-1.9406e+00,  2.4725e+00,  1.6705e+00],\n",
       "        [-9.8742e-01,  3.8218e-01,  6.4504e-01],\n",
       "        [-1.5450e+00,  6.5301e-02,  1.3827e+00],\n",
       "        [-3.5991e-01,  8.1729e-01,  9.9629e-01],\n",
       "        [ 1.1247e+00, -4.1395e-01, -5.2273e-01],\n",
       "        [ 1.1028e+00, -5.0212e-01, -1.7787e-01],\n",
       "        [ 1.4010e+00, -8.1900e-01, -2.6039e-01],\n",
       "        [ 1.8567e-01, -7.8566e-01, -3.9485e-01],\n",
       "        [ 1.0550e+00, -1.1905e+00, -1.7709e-01],\n",
       "        [ 8.4066e-01, -1.2324e+00, -5.7564e-01],\n",
       "        [ 9.2758e-01, -1.0143e+00, -5.3258e-01],\n",
       "        [ 1.1009e+00, -1.0509e+00, -3.0520e-01],\n",
       "        [ 1.4156e+00, -1.6791e+00, -1.1323e-01],\n",
       "        [ 1.6575e+00, -8.6186e-01,  2.9070e-01],\n",
       "        [ 1.0220e+00, -1.7001e+00, -2.3801e-01],\n",
       "        [ 6.3857e-01, -1.0620e+00,  4.3835e-01],\n",
       "        [ 6.2901e-01, -9.2963e-01,  1.7054e-01],\n",
       "        [ 1.3519e+00, -1.4083e+00, -6.7754e-01],\n",
       "        [ 6.7330e-01, -9.6653e-01, -3.8663e-01],\n",
       "        [ 2.3415e-01, -8.9601e-01, -6.8454e-01],\n",
       "        [ 1.5111e+00, -1.6660e+00, -2.8023e-01],\n",
       "        [ 1.5448e+00, -1.1622e+00, -2.6715e-02],\n",
       "        [ 8.3098e-01, -1.3198e+00, -6.4212e-01],\n",
       "        [-8.5998e-02,  7.3360e-02,  5.3868e-01]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_features_tensor = torch.from_numpy(wine_features_norm).float()\n",
    "# display(wine_features_tensor)\n",
    "model.forward(wine_features_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.state_dict()\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1736, 0.5839, 0.2425],\n",
       "        [0.2892, 0.5328, 0.1780],\n",
       "        [0.1139, 0.5276, 0.3585],\n",
       "        [0.1563, 0.6207, 0.2230],\n",
       "        [0.1577, 0.5862, 0.2561],\n",
       "        [0.0789, 0.7753, 0.1458],\n",
       "        [0.2125, 0.5031, 0.2845],\n",
       "        [0.4070, 0.4257, 0.1673],\n",
       "        [0.3347, 0.5262, 0.1391],\n",
       "        [0.2418, 0.4429, 0.3153],\n",
       "        [0.2472, 0.4544, 0.2984],\n",
       "        [0.1504, 0.6446, 0.2050],\n",
       "        [0.3059, 0.5203, 0.1739],\n",
       "        [0.2795, 0.4983, 0.2222],\n",
       "        [0.1594, 0.6956, 0.1451],\n",
       "        [0.2615, 0.3316, 0.4069],\n",
       "        [0.1264, 0.5685, 0.3051],\n",
       "        [0.1652, 0.6633, 0.1715],\n",
       "        [0.1164, 0.5758, 0.3078],\n",
       "        [0.1063, 0.6887, 0.2049],\n",
       "        [0.4487, 0.1325, 0.4188],\n",
       "        [0.4412, 0.2230, 0.3358],\n",
       "        [0.2479, 0.1588, 0.5934],\n",
       "        [0.4569, 0.2056, 0.3375],\n",
       "        [0.5865, 0.1304, 0.2832],\n",
       "        [0.6166, 0.2045, 0.1789],\n",
       "        [0.6649, 0.1494, 0.1857],\n",
       "        [0.5859, 0.1768, 0.2372],\n",
       "        [0.5854, 0.1722, 0.2424],\n",
       "        [0.6242, 0.1671, 0.2087],\n",
       "        [0.3013, 0.1359, 0.5628],\n",
       "        [0.6999, 0.1981, 0.1020],\n",
       "        [0.3327, 0.1069, 0.5603],\n",
       "        [0.4182, 0.1416, 0.4403],\n",
       "        [0.5166, 0.0799, 0.4035],\n",
       "        [0.2925, 0.0561, 0.6514],\n",
       "        [0.6393, 0.1239, 0.2368],\n",
       "        [0.6030, 0.2208, 0.1763],\n",
       "        [0.5885, 0.0589, 0.3526],\n",
       "        [0.5610, 0.1898, 0.2491],\n",
       "        [0.3115, 0.3195, 0.3689],\n",
       "        [0.4947, 0.2107, 0.2946],\n",
       "        [0.4869, 0.2352, 0.2778],\n",
       "        [0.3619, 0.4560, 0.1820],\n",
       "        [0.3733, 0.4243, 0.2024],\n",
       "        [0.2968, 0.2731, 0.4301],\n",
       "        [0.4535, 0.3273, 0.2192],\n",
       "        [0.3977, 0.4552, 0.1471],\n",
       "        [0.2956, 0.4095, 0.2949],\n",
       "        [0.1996, 0.5330, 0.2674],\n",
       "        [0.3269, 0.5091, 0.1640],\n",
       "        [0.2045, 0.4130, 0.3825],\n",
       "        [0.3070, 0.4342, 0.2588],\n",
       "        [0.1734, 0.6000, 0.2266],\n",
       "        [0.2844, 0.4156, 0.3001],\n",
       "        [0.1881, 0.5616, 0.2503],\n",
       "        [0.1877, 0.5083, 0.3040],\n",
       "        [0.2322, 0.4834, 0.2844],\n",
       "        [0.3034, 0.3544, 0.3421],\n",
       "        [0.0661, 0.7574, 0.1764],\n",
       "        [0.0781, 0.6526, 0.2693],\n",
       "        [0.1655, 0.5955, 0.2390],\n",
       "        [0.0797, 0.4124, 0.5079],\n",
       "        [0.1005, 0.6940, 0.2056],\n",
       "        [0.3718, 0.4539, 0.1743],\n",
       "        [0.2987, 0.5974, 0.1038],\n",
       "        [0.2120, 0.6346, 0.1533],\n",
       "        [0.1122, 0.7171, 0.1707],\n",
       "        [0.2164, 0.4059, 0.3777],\n",
       "        [0.2299, 0.6063, 0.1638],\n",
       "        [0.2806, 0.3455, 0.3738],\n",
       "        [0.1955, 0.5707, 0.2338],\n",
       "        [0.2717, 0.3995, 0.3288],\n",
       "        [0.1216, 0.7250, 0.1534],\n",
       "        [0.1542, 0.5057, 0.3402],\n",
       "        [0.1667, 0.2954, 0.5379],\n",
       "        [0.1299, 0.6652, 0.2049],\n",
       "        [0.1826, 0.5764, 0.2410],\n",
       "        [0.4532, 0.0432, 0.5036],\n",
       "        [0.3288, 0.3244, 0.3468],\n",
       "        [0.6026, 0.1696, 0.2279],\n",
       "        [0.5387, 0.2111, 0.2502],\n",
       "        [0.3122, 0.5692, 0.1186],\n",
       "        [0.3788, 0.4477, 0.1735],\n",
       "        [0.3777, 0.1625, 0.4598],\n",
       "        [0.3100, 0.0767, 0.6133],\n",
       "        [0.6143, 0.1068, 0.2789],\n",
       "        [0.4061, 0.3052, 0.2887],\n",
       "        [0.6823, 0.1979, 0.1198],\n",
       "        [0.5272, 0.1122, 0.3605],\n",
       "        [0.4089, 0.1638, 0.4273],\n",
       "        [0.5987, 0.1382, 0.2632],\n",
       "        [0.5347, 0.1497, 0.3156],\n",
       "        [0.5807, 0.2385, 0.1808],\n",
       "        [0.5056, 0.2022, 0.2922],\n",
       "        [0.4847, 0.0871, 0.4283],\n",
       "        [0.5901, 0.1984, 0.2115],\n",
       "        [0.5408, 0.2648, 0.1945],\n",
       "        [0.6523, 0.1055, 0.2422],\n",
       "        [0.4234, 0.1586, 0.4180],\n",
       "        [0.5199, 0.1044, 0.3757],\n",
       "        [0.3932, 0.2626, 0.3443],\n",
       "        [0.5029, 0.2749, 0.2222],\n",
       "        [0.3791, 0.4134, 0.2075],\n",
       "        [0.4312, 0.2639, 0.3049],\n",
       "        [0.4982, 0.3217, 0.1801],\n",
       "        [0.3924, 0.4193, 0.1882],\n",
       "        [0.3152, 0.3588, 0.3260],\n",
       "        [0.3098, 0.2819, 0.4084],\n",
       "        [0.2500, 0.4227, 0.3273],\n",
       "        [0.3048, 0.4200, 0.2752],\n",
       "        [0.3497, 0.2716, 0.3787],\n",
       "        [0.4198, 0.3061, 0.2741],\n",
       "        [0.3811, 0.3186, 0.3003],\n",
       "        [0.2543, 0.3964, 0.3493],\n",
       "        [0.5374, 0.2085, 0.2541],\n",
       "        [0.3526, 0.3309, 0.3164],\n",
       "        [0.1880, 0.5453, 0.2667],\n",
       "        [0.1607, 0.6171, 0.2222],\n",
       "        [0.1552, 0.7203, 0.1245],\n",
       "        [0.0873, 0.6681, 0.2446],\n",
       "        [0.0654, 0.5427, 0.3919],\n",
       "        [0.1264, 0.7282, 0.1454],\n",
       "        [0.0389, 0.7878, 0.1734],\n",
       "        [0.2305, 0.6028, 0.1668],\n",
       "        [0.2674, 0.5247, 0.2079],\n",
       "        [0.2152, 0.5870, 0.1978],\n",
       "        [0.0966, 0.6998, 0.2036],\n",
       "        [0.2287, 0.5725, 0.1989],\n",
       "        [0.2049, 0.3979, 0.3972],\n",
       "        [0.1895, 0.5156, 0.2949],\n",
       "        [0.1242, 0.5729, 0.3029],\n",
       "        [0.0764, 0.6532, 0.2704],\n",
       "        [0.1773, 0.5900, 0.2327],\n",
       "        [0.1529, 0.5590, 0.2881],\n",
       "        [0.1565, 0.6494, 0.1942],\n",
       "        [0.3381, 0.1613, 0.5005],\n",
       "        [0.3951, 0.2651, 0.3398],\n",
       "        [0.3743, 0.2664, 0.3593],\n",
       "        [0.5078, 0.0828, 0.4093],\n",
       "        [0.2513, 0.6346, 0.1140],\n",
       "        [0.5584, 0.2376, 0.2040],\n",
       "        [0.5249, 0.0917, 0.3835],\n",
       "        [0.3957, 0.2804, 0.3239],\n",
       "        [0.2663, 0.2820, 0.4517],\n",
       "        [0.5307, 0.1723, 0.2970],\n",
       "        [0.6051, 0.1441, 0.2508],\n",
       "        [0.6716, 0.1337, 0.1947],\n",
       "        [0.4617, 0.1212, 0.4171],\n",
       "        [0.2983, 0.2688, 0.4329],\n",
       "        [0.4979, 0.1573, 0.3448],\n",
       "        [0.4288, 0.2398, 0.3314],\n",
       "        [0.5306, 0.2211, 0.2484],\n",
       "        [0.5535, 0.1841, 0.2624],\n",
       "        [0.2492, 0.6636, 0.0872],\n",
       "        [0.4975, 0.3399, 0.1627],\n",
       "        [0.5215, 0.1959, 0.2826],\n",
       "        [0.5261, 0.1926, 0.2812],\n",
       "        [0.4326, 0.3114, 0.2560],\n",
       "        [0.3799, 0.1242, 0.4959],\n",
       "        [0.3031, 0.1951, 0.5017],\n",
       "        [0.3786, 0.3186, 0.3027],\n",
       "        [0.3546, 0.3378, 0.3076],\n",
       "        [0.3161, 0.4276, 0.2562],\n",
       "        [0.1775, 0.6919, 0.1306],\n",
       "        [0.2545, 0.5523, 0.1932],\n",
       "        [0.1659, 0.2570, 0.5771],\n",
       "        [0.3224, 0.1763, 0.5012],\n",
       "        [0.1871, 0.3231, 0.4898],\n",
       "        [0.1072, 0.5952, 0.2976],\n",
       "        [0.1981, 0.4349, 0.3671],\n",
       "        [0.2074, 0.5388, 0.2538],\n",
       "        [0.2169, 0.6150, 0.1681],\n",
       "        [0.2039, 0.7079, 0.0881],\n",
       "        [0.1734, 0.4506, 0.3760],\n",
       "        [0.2194, 0.3658, 0.4148],\n",
       "        [0.1565, 0.6306, 0.2128],\n",
       "        [0.5269, 0.1713, 0.3018]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WineNet()\n",
    "model.forward_softmax(wine_features_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SoftMax function just converts the values into probabilities. So, we are given three probabilities in the output tensor, with each probability corresponding to a certain category (in this case, the probability that this data point belongs in which cultivar, 1 2 or 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b\n",
    "\n",
    "*For debugging*: The accuracy could reach over 95\\% if the hyperparamters are tuned properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# you can use this framework to do training and validation           \n",
    "def train_and_val(model,train_X,train_y,epochs,draw_curve=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------\n",
    "    model: a PyTorch model\n",
    "    train_X: np.array shape(ndata,nfeatures)\n",
    "    train_y: np.array shape(ndata)\n",
    "    epochs: int\n",
    "    draw_curve: bool\n",
    "    \"\"\"\n",
    "    ### Define your loss function, optimizer. Convert data to torch tensor ###\n",
    "    optimizer = Adam(model.parameters(), lr=5E-2)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    train_X = torch.tensor(train_X, dtype=torch.float)\n",
    "    # train_y = train_y - 1\n",
    "    train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "    \n",
    "    ### Split training examples further into training and validation ###\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2)\n",
    "\n",
    "    val_array=[]\n",
    "    lowest_val_loss = np.inf\n",
    "    model_param = model.state_dict()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        ### Compute the loss and do backpropagation ###\n",
    "        # display(X_train.dtype)\n",
    "        y_pred = model(X_train)\n",
    "        loss = loss_func(y_pred, y_train-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### compute validation loss and keep track of the lowest val loss ###\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(X_val)\n",
    "            # print(y_hat)\n",
    "            # print(y_val)\n",
    "            val_loss = loss_func(model(X_val), y_val-1)\n",
    "            val_array.append(val_loss)\n",
    "            \n",
    "            if val_loss < lowest_val_loss:\n",
    "                lowest_val_loss = val_loss\n",
    "                model_param = model.state_dict()\n",
    "                \n",
    "     # The final number of epochs is when the minimum error in validation set occurs    \n",
    "    final_epochs=np.argmin(val_array)+1\n",
    "    print(\"Number of epochs with lowest validation:\",final_epochs)\n",
    "    ### Recover the model weight ###\n",
    "    model.load_state_dict(model_param)\n",
    "\n",
    "    if draw_curve:\n",
    "        plt.plot(np.arange(len(val_array))+1,val_array,label='Validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs with lowest validation: 69\n",
      "Train accuracy: 0.9830508\n",
      "Test accuracy: 0.95\n",
      "Number of epochs with lowest validation: 500\n",
      "Train accuracy: 0.99159664\n",
      "Test accuracy: 0.9830508\n",
      "Number of epochs with lowest validation: 30\n",
      "Train accuracy: 0.96638656\n",
      "Test accuracy: 0.9322034\n",
      "Final results:\n",
      "Training accuracy mean: 0.9803447127342224, std: 0.010468349792063236\n",
      "Testing  accuracy mean: 0.9550848007202148, std: 0.02106744423508644\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_accuracy_nn(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        y_pred = torch.argmax(model(X), axis=1)\n",
    "        acc = torch.sum(y_pred == y) / len(y)\n",
    "    return acc.detach().numpy()\n",
    "\n",
    "def KFoldNN(k, X, y, epochs=500):\n",
    "    \"\"\"\n",
    "    K-Fold Validation for Neural Network\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    k: int\n",
    "        Number of folds\n",
    "    X: numpy.ndarray\n",
    "        Input data, shape (n_samples, n_features)\n",
    "    y: numpy.ndarray\n",
    "        Class labels, shape (n_samples)\n",
    "    epochs: int\n",
    "        Number of epochs during training\n",
    "    \"\"\"\n",
    "    # K-Fold\n",
    "    kf = KFold(n_splits=3)\n",
    "    train_acc_all = []\n",
    "    test_acc_all = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # X_train, X_test = torch.from_numpy(X[train_index]), torch.from_numpy(X[test_index])\n",
    "        # y_train, y_test = torch.from_numpy(y[train_index]), torch.from_numpy(y[test_index])\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # further do a train/valid split on X_train\n",
    "        model = WineNet()\n",
    "        train_and_val(model, X_train, y_train, epochs)\n",
    "        \n",
    "        # Report prediction accuracy for this fold\n",
    "        # use calculate_accuracy_nn() function\n",
    "        train_acc = calculate_accuracy_nn(model, torch.from_numpy(X_train).float(), torch.from_numpy(y_train-1).float())\n",
    "        train_acc_all.append(train_acc)\n",
    "        test_acc = calculate_accuracy_nn(model, torch.from_numpy(X_test).float(), torch.from_numpy(y_test-1).float())\n",
    "        test_acc_all.append(test_acc)\n",
    "        print(\"Train accuracy:\", train_acc)\n",
    "        print(\"Test accuracy:\", test_acc)\n",
    "    \n",
    "    # report mean & std for the training/testing accuracy\n",
    "    print(\"Final results:\")\n",
    "    print(f\"Training accuracy mean: {np.mean(train_acc_all)}, std: {np.std(train_acc_all)}\")\n",
    "    print(f\"Testing  accuracy mean: {np.mean(test_acc_all)}, std: {np.std(test_acc_all)}\")\n",
    "\n",
    "KFoldNN(3, wine_features_np, wine_rankings_np)\n",
    "# KFoldNN(3, wine_features_tensor, wine_rankings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
